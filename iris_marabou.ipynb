{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_X033oPFf-0",
    "outputId": "dc284c94-5a9f-4bd2-c476-d16aa57c5ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.17.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.25.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: onnx2pytorch in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (2.1.0+cu121)\n",
      "Requirement already satisfied: onnx>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (1.15.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (0.16.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.6.0->onnx2pytorch) (1.25.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.6.0->onnx2pytorch) (3.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->onnx2pytorch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.0->onnx2pytorch) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->onnx2pytorch) (1.3.0)\n",
      "Requirement already satisfied: maraboupy in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxruntime\n",
    "!pip install onnx2pytorch\n",
    "!pip install maraboupy\n",
    "#!pip install tensorflow[and-cuda]==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YILcoWdCMO1"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82fo7FO0v9R_",
    "outputId": "a0407866-c6ef-471a-f573-7422c949016d"
   },
   "outputs": [],
   "source": [
    "\n",
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "\n",
    "iris_onnx_path =  'iris_model_60.onnx'  # Adjust file extension if needed\n",
    "PATTERN_PATH = 'relu_states'  \n",
    "\n",
    "# Adjust file extension if needed\n",
    "# onnx_model = onnx.load(iris_onnx_path)\n",
    "# model = ConvertModel(onnx_model)\n",
    "# data=iris.data\n",
    "# data\n",
    "# X_train\n",
    "#labels=iris.target #target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10' - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RZmiALnbf9Hd"
   },
   "outputs": [],
   "source": [
    "from maraboupy import Marabou, MarabouCore, MarabouUtils\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "import logging\n",
    "import pandas\n",
    "\n",
    "def init_network():\n",
    "  network = Marabou.read_onnx(iris_onnx_path)\n",
    "  return network\n",
    "\n",
    "EPSILON = 0.5\n",
    "MAX_TIME = 300 #in seconds\n",
    "M_OPTIONS: MarabouCore.Options = Marabou.createOptions(verbosity=0, numWorkers=10, timeoutInSeconds=MAX_TIME)\n",
    "\n",
    "def convert_keys_to_int(x):\n",
    "    if isinstance(x, dict):\n",
    "        return {int(k) if k.isdigit() else k: convert_keys_to_int(v) for k, v in x.items()}\n",
    "    return x\n",
    "\n",
    "def parse_raw_idx(raw_idx: int) -> Tuple[int, int, int]:\n",
    "    n_relus = 20\n",
    "    offset = 7\n",
    "    layer = raw_idx // n_relus\n",
    "    idx = raw_idx % n_relus\n",
    "    marabou_idx = 2*n_relus*layer + idx + offset\n",
    "    return layer, idx, marabou_idx\n",
    "\n",
    "with open(PATTERN_PATH, \"r\") as f:\n",
    "  STABLE_PATTERNS = json.load(f)\n",
    "  STABLE_PATTERNS=convert_keys_to_int(STABLE_PATTERNS)\n",
    "loc = 0.5\n",
    "radus = 0.5\n",
    "non_restricted_dim = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YceIZIgtC5Fn"
   },
   "outputs": [],
   "source": [
    "for k,v in STABLE_PATTERNS.items():\n",
    "  print(f'label={k}')\n",
    "  for neuron,values in v.items():\n",
    "    print(f\"neuron={neuron}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M98VjGpkwzeN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.7110157608985901\n",
      "Epoch 20, Loss: 0.4036754369735718\n",
      "Epoch 30, Loss: 0.3703729510307312\n",
      "Epoch 40, Loss: 0.14934490621089935\n",
      "Epoch 50, Loss: 0.12423625588417053\n",
      "Epoch 60, Loss: 0.005594757851213217\n",
      "Epoch 70, Loss: 0.08966691792011261\n",
      "Epoch 80, Loss: 0.002957663033157587\n",
      "Epoch 90, Loss: 0.020865704864263535\n",
      "Epoch 100, Loss: 0.08964332938194275\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 20)  # Input layer to hidden layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 20) # Hidden layer to hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(20, 20) # Hidden layer to hidden layer\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(20, 3)  # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "model = IrisNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Check if the model trains without error up to this point\n",
    "# \"Model trained successfully!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqDgTtJARaCI",
    "outputId": "ca02ca74-4821-43c8-8fc9-83b9873f921d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relu1': tensor([[1.2062, 1.4942, 0.0000, 2.6629, 0.0000, 0.0000, 0.1008, 0.0000, 1.2812,\n",
       "          0.4878, 1.9803, 1.7296, 2.2217, 1.8856, 0.0000, 0.0000, 1.3994, 0.0000,\n",
       "          0.0000, 2.1766]]),\n",
       " 'relu2': tensor([[3.5583e+00, 0.0000e+00, 4.5825e+00, 0.0000e+00, 3.0476e+00, 3.5773e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.5550e+00, 3.8430e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2207e+00, 3.7258e+00, 5.4662e+00,\n",
       "          0.0000e+00, 1.4705e-03]]),\n",
       " 'relu3': tensor([[ 0.0000,  0.0000,  0.0000,  2.0140,  0.0000,  3.8408,  0.0000,  0.0000,\n",
       "           3.7716,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.4726,  8.1664,\n",
       "           0.0000,  0.0000,  8.5828,  0.0000]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "\n",
    "# onnx_model = onnx.load(\"iris_model.onnx\")\n",
    "# model = ConvertModel(onnx_model)\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "i=1\n",
    "# Assuming the ReLU layers are named similarly to your PyTorch model, you can add hooks like this\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, torch.nn.ReLU):\n",
    "        # Register the hook\n",
    "        layer.register_forward_hook(get_activation(f'relu{i}'))\n",
    "        i=i+1\n",
    "\n",
    "# Now, when you run a forward pass, the hooks will store the ReLU activations\n",
    "with torch.no_grad():\n",
    "    output = model(X_train_tensor[0].unsqueeze(0))\n",
    "\n",
    "# Check the captured activations\n",
    "activations\n",
    "# Remove hooks (to clean up)\n",
    "# hook1.remove()\n",
    "# hook2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZaXSAFSh5Nu",
    "outputId": "5c0773aa-d716-4291-e12e-f6e9adbb23da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30\n"
     ]
    }
   ],
   "source": [
    "#accuracy:\n",
    "i=0\n",
    "with torch.no_grad():\n",
    "  for inputs,label in zip(X_test_tensor, y_test_tensor):\n",
    "          outputs = model(inputs.unsqueeze(0))\n",
    "          #print(outputs)\n",
    "          pred=outputs.argmax() #output not normalized\n",
    "          if pred==label:\n",
    "            i=i+1\n",
    "print(f'{i}/{len(X_test_tensor)}')\n",
    "\n",
    "model(X_train_tensor[0].unsqueeze(0))\n",
    "\n",
    "onnx_file_path = \"iris_model_60.onnx\"\n",
    "torch.onnx.export(model, torch.randn(1, 4), onnx_file_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8IRdRkQ-pOl"
   },
   "source": [
    "find states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QkaVL6lNSVwH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.828767716884613,\n",
       "  0.6903653144836426,\n",
       "  0.6370339393615723,\n",
       "  1.246646523475647,\n",
       "  0.8410075306892395,\n",
       "  1.0450611114501953,\n",
       "  1.4888828992843628,\n",
       "  0.0,\n",
       "  0.7377094626426697,\n",
       "  0.7998632788658142,\n",
       "  1.1056742668151855,\n",
       "  1.649401307106018,\n",
       "  0.8751844167709351,\n",
       "  0.7333645820617676,\n",
       "  1.2010773420333862,\n",
       "  1.5152419805526733,\n",
       "  0.9550075531005859,\n",
       "  1.0494059324264526,\n",
       "  0.9167447090148926,\n",
       "  0.5503205060958862,\n",
       "  0.6676362156867981,\n",
       "  1.1625556945800781,\n",
       "  0.750876784324646,\n",
       "  0.5013340711593628,\n",
       "  1.1982731819152832,\n",
       "  0.6580435037612915,\n",
       "  0.7823510766029358,\n",
       "  0.6723171472549438,\n",
       "  0.8734093308448792,\n",
       "  1.2464121580123901,\n",
       "  0.6151769161224365,\n",
       "  0.40757331252098083,\n",
       "  1.0747358798980713,\n",
       "  0.9137061238288879,\n",
       "  0.9532325267791748,\n",
       "  1.0205014944076538,\n",
       "  1.2062480449676514,\n",
       "  0.45926231145858765,\n",
       "  0.8200780153274536,\n",
       "  0.9129357933998108],\n",
       " 1: [1.266777515411377,\n",
       "  1.1708238124847412,\n",
       "  1.143419861793518,\n",
       "  1.1619681119918823,\n",
       "  1.940536379814148,\n",
       "  1.3608477115631104,\n",
       "  1.521022081375122,\n",
       "  1.5156525373458862,\n",
       "  0.68492591381073,\n",
       "  1.727431297302246,\n",
       "  1.2022740840911865,\n",
       "  1.1248717308044434,\n",
       "  0.929201602935791,\n",
       "  1.2473864555358887,\n",
       "  1.2538753747940063,\n",
       "  0.9794766902923584,\n",
       "  0.927594780921936,\n",
       "  1.8072762489318848,\n",
       "  0.8093271255493164,\n",
       "  1.4250715970993042,\n",
       "  0.937010645866394,\n",
       "  1.7220617532730103,\n",
       "  1.3605681657791138,\n",
       "  0.6534786820411682,\n",
       "  1.0987879037857056,\n",
       "  0.8835259675979614,\n",
       "  0.5333245992660522,\n",
       "  1.312453031539917,\n",
       "  1.4731836318969727,\n",
       "  1.4941742420196533,\n",
       "  1.1880488395690918,\n",
       "  1.4917347431182861,\n",
       "  1.060368299484253,\n",
       "  1.2068039178848267,\n",
       "  1.652188777923584,\n",
       "  1.2796765565872192,\n",
       "  1.1909759044647217,\n",
       "  0.8233455419540405,\n",
       "  1.148789405822754,\n",
       "  0.9455838799476624],\n",
       " 2: [0.0,\n",
       "  0.04803657531738281,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4165295958518982,\n",
       "  0.1212189793586731,\n",
       "  0.0,\n",
       "  0.2269478440284729,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6656107306480408,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.37415599822998047,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.12910127639770508,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.014735877513885498],\n",
       " 3: [2.012356996536255,\n",
       "  2.1277005672454834,\n",
       "  1.985581398010254,\n",
       "  1.9189352989196777,\n",
       "  2.2558200359344482,\n",
       "  1.8635733127593994,\n",
       "  1.9593284130096436,\n",
       "  2.118605375289917,\n",
       "  2.559872627258301,\n",
       "  1.637089729309082,\n",
       "  2.6864962577819824,\n",
       "  1.9039664268493652,\n",
       "  2.153162956237793,\n",
       "  2.0887060165405273,\n",
       "  1.9144599437713623,\n",
       "  2.1762535572052,\n",
       "  2.217254638671875,\n",
       "  1.9840004444122314,\n",
       "  2.265199899673462,\n",
       "  2.3574485778808594,\n",
       "  1.4919910430908203,\n",
       "  1.8583076000213623,\n",
       "  2.247192144393921,\n",
       "  1.1012401580810547,\n",
       "  2.0325534343719482,\n",
       "  1.9011695384979248,\n",
       "  1.7103147506713867,\n",
       "  2.049065351486206,\n",
       "  2.0225274562835693,\n",
       "  2.337719678878784,\n",
       "  2.0632436275482178,\n",
       "  2.1264426708221436,\n",
       "  1.6302809715270996,\n",
       "  1.609153389930725,\n",
       "  2.5859429836273193,\n",
       "  2.6628525257110596,\n",
       "  2.2411742210388184,\n",
       "  2.067718744277954,\n",
       "  1.8792946338653564,\n",
       "  2.1913628578186035],\n",
       " 4: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 5: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.05117669701576233,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.16383260488510132,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4270865321159363,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.24622851610183716,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 6: [0.02391570806503296,\n",
       "  0.07286274433135986,\n",
       "  0.23260116577148438,\n",
       "  0.25481945276260376,\n",
       "  0.1656622290611267,\n",
       "  0.12721675634384155,\n",
       "  0.23485684394836426,\n",
       "  0.0,\n",
       "  0.15429240465164185,\n",
       "  0.24652260541915894,\n",
       "  0.0,\n",
       "  0.07210719585418701,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.31634825468063354,\n",
       "  0.08104681968688965,\n",
       "  0.08902806043624878,\n",
       "  0.21747636795043945,\n",
       "  0.06568795442581177,\n",
       "  0.08150643110275269,\n",
       "  0.2773532271385193,\n",
       "  0.01931208372116089,\n",
       "  0.2947728633880615,\n",
       "  0.11761480569839478,\n",
       "  0.06007498502731323,\n",
       "  0.003606259822845459,\n",
       "  0.030681908130645752,\n",
       "  0.10080665349960327,\n",
       "  0.10812246799468994,\n",
       "  0.21263855695724487,\n",
       "  0.1500973105430603,\n",
       "  0.23294800519943237,\n",
       "  0.28331297636032104,\n",
       "  0.0,\n",
       "  0.03785675764083862,\n",
       "  0.30280500650405884,\n",
       "  0.07089215517044067,\n",
       "  0.26598337292671204,\n",
       "  0.0],\n",
       " 7: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 8: [1.2812013626098633,\n",
       "  1.6050927639007568,\n",
       "  0.9601218700408936,\n",
       "  0.7876605987548828,\n",
       "  0.2790970206260681,\n",
       "  1.9579131603240967,\n",
       "  0.8476023077964783,\n",
       "  1.7273995876312256,\n",
       "  1.273303508758545,\n",
       "  1.2047357559204102,\n",
       "  1.3394477367401123,\n",
       "  1.1695146560668945,\n",
       "  1.003240704536438,\n",
       "  1.058854103088379,\n",
       "  0.9110982418060303,\n",
       "  0.47373542189598083,\n",
       "  1.3580553531646729,\n",
       "  1.9850571155548096,\n",
       "  0.7234058380126953,\n",
       "  0.8036351799964905,\n",
       "  1.4933013916015625,\n",
       "  0.2234683632850647,\n",
       "  1.8769402503967285,\n",
       "  0.448925256729126,\n",
       "  0.7536750435829163,\n",
       "  1.5618691444396973,\n",
       "  1.1404809951782227,\n",
       "  1.04899263381958,\n",
       "  1.534635305404663,\n",
       "  1.686065435409546,\n",
       "  1.335668683052063,\n",
       "  1.0612928867340088,\n",
       "  0.0,\n",
       "  1.0595080852508545,\n",
       "  0.6091015338897705,\n",
       "  1.5100195407867432,\n",
       "  1.0014560222625732,\n",
       "  1.3992846012115479,\n",
       "  1.070023775100708,\n",
       "  2.2798101902008057],\n",
       " 9: [0.5367746353149414,\n",
       "  0.6285180449485779,\n",
       "  1.383596658706665,\n",
       "  0.9950289726257324,\n",
       "  2.0446746349334717,\n",
       "  0.0,\n",
       "  0.8686721324920654,\n",
       "  0.4518972933292389,\n",
       "  1.2027021646499634,\n",
       "  1.1882362365722656,\n",
       "  0.4878440201282501,\n",
       "  0.7452051639556885,\n",
       "  0.7983142733573914,\n",
       "  0.6271311044692993,\n",
       "  0.6775438189506531,\n",
       "  0.0,\n",
       "  0.7099831104278564,\n",
       "  0.8038471341133118,\n",
       "  0.2607153654098511,\n",
       "  0.46569210290908813,\n",
       "  0.0,\n",
       "  1.1916900873184204,\n",
       "  0.47467872500419617,\n",
       "  0.5554190278053284,\n",
       "  0.18342888355255127,\n",
       "  1.4677493572235107,\n",
       "  1.0923469066619873,\n",
       "  0.2282760739326477,\n",
       "  0.6216519474983215,\n",
       "  1.094372272491455,\n",
       "  0.8742585182189941,\n",
       "  0.0,\n",
       "  1.3269801139831543,\n",
       "  0.15992268919944763,\n",
       "  0.2317299246788025,\n",
       "  0.0,\n",
       "  1.0509209632873535,\n",
       "  0.0,\n",
       "  0.33178895711898804,\n",
       "  0.5277879238128662],\n",
       " 10: [1.6552540063858032,\n",
       "  1.7904289960861206,\n",
       "  1.8708246946334839,\n",
       "  1.7474974393844604,\n",
       "  1.6994715929031372,\n",
       "  1.5126595497131348,\n",
       "  1.738684058189392,\n",
       "  1.8214542865753174,\n",
       "  1.9802844524383545,\n",
       "  1.7448176145553589,\n",
       "  2.0566558837890625,\n",
       "  1.8849974870681763,\n",
       "  1.8034144639968872,\n",
       "  1.6705553531646729,\n",
       "  1.7059595584869385,\n",
       "  1.8422725200653076,\n",
       "  1.5942425727844238,\n",
       "  1.3992434740066528,\n",
       "  1.9123624563217163,\n",
       "  1.8434597253799438,\n",
       "  1.6061302423477173,\n",
       "  1.7201324701309204,\n",
       "  1.9127168655395508,\n",
       "  1.6470081806182861,\n",
       "  1.595312476158142,\n",
       "  1.4082046747207642,\n",
       "  1.6695747375488281,\n",
       "  1.740950584411621,\n",
       "  1.9320424795150757,\n",
       "  1.9514271020889282,\n",
       "  1.722605586051941,\n",
       "  1.582444190979004,\n",
       "  1.7032798528671265,\n",
       "  1.7979563474655151,\n",
       "  1.8176459074020386,\n",
       "  1.5398179292678833,\n",
       "  1.7565172910690308,\n",
       "  1.7343052625656128,\n",
       "  1.508487343788147,\n",
       "  1.6915801763534546],\n",
       " 11: [1.3278417587280273,\n",
       "  1.4437024593353271,\n",
       "  1.5208103656768799,\n",
       "  1.8790608644485474,\n",
       "  1.6891525983810425,\n",
       "  1.1787316799163818,\n",
       "  1.4047672748565674,\n",
       "  1.6848790645599365,\n",
       "  0.9367925524711609,\n",
       "  1.5761878490447998,\n",
       "  1.7877205610275269,\n",
       "  1.3384343385696411,\n",
       "  1.247849702835083,\n",
       "  1.4307893514633179,\n",
       "  1.199066162109375,\n",
       "  1.8274879455566406,\n",
       "  1.6120445728302002,\n",
       "  1.626078486442566,\n",
       "  1.4724961519241333,\n",
       "  1.2190378904342651,\n",
       "  1.4479758739471436,\n",
       "  1.975114107131958,\n",
       "  1.6790292263031006,\n",
       "  1.4690546989440918,\n",
       "  1.281661033630371,\n",
       "  1.729645848274231,\n",
       "  1.0790382623672485,\n",
       "  1.2851204872131348,\n",
       "  1.5601085424423218,\n",
       "  1.5960533618927002,\n",
       "  1.141749382019043,\n",
       "  1.5321470499038696,\n",
       "  1.4580992460250854,\n",
       "  1.4822564125061035,\n",
       "  1.5317840576171875,\n",
       "  1.393811821937561,\n",
       "  1.2854835987091064,\n",
       "  1.1817917823791504,\n",
       "  1.8173644542694092,\n",
       "  1.3197640180587769],\n",
       " 12: [1.7041525840759277,\n",
       "  2.4395687580108643,\n",
       "  1.9721806049346924,\n",
       "  2.232621908187866,\n",
       "  1.696007251739502,\n",
       "  2.221741199493408,\n",
       "  2.230445623397827,\n",
       "  2.002413272857666,\n",
       "  1.4752742052078247,\n",
       "  1.8732805252075195,\n",
       "  1.5365195274353027,\n",
       "  1.9132757186889648,\n",
       "  1.945684552192688,\n",
       "  0.9802718162536621,\n",
       "  2.1062722206115723,\n",
       "  2.1753342151641846,\n",
       "  1.7992594242095947,\n",
       "  1.7441478967666626,\n",
       "  2.431145429611206,\n",
       "  1.2699443101882935,\n",
       "  1.808011770248413,\n",
       "  1.7970833778381348,\n",
       "  1.2113264799118042,\n",
       "  1.3455826044082642,\n",
       "  1.5879602432250977,\n",
       "  2.006042003631592,\n",
       "  1.5052765607833862,\n",
       "  2.4417450428009033,\n",
       "  1.7150812149047852,\n",
       "  1.6596406698226929,\n",
       "  1.539377212524414,\n",
       "  2.397397518157959,\n",
       "  1.5719980001449585,\n",
       "  2.050554037094116,\n",
       "  1.938713788986206,\n",
       "  2.633411169052124,\n",
       "  1.3533991575241089,\n",
       "  2.010558605194092,\n",
       "  1.8500187397003174,\n",
       "  1.5139391422271729],\n",
       " 13: [1.811711072921753,\n",
       "  1.9563323259353638,\n",
       "  1.8856319189071655,\n",
       "  2.1507134437561035,\n",
       "  1.8105288743972778,\n",
       "  1.8351309299468994,\n",
       "  1.570860743522644,\n",
       "  1.9630341529846191,\n",
       "  1.6704504489898682,\n",
       "  1.6920621395111084,\n",
       "  1.826771855354309,\n",
       "  1.643669843673706,\n",
       "  1.8848906755447388,\n",
       "  1.619253158569336,\n",
       "  1.6688276529312134,\n",
       "  1.7263562679290771,\n",
       "  1.6997950077056885,\n",
       "  1.8835232257843018,\n",
       "  1.592879056930542,\n",
       "  1.8833379745483398,\n",
       "  1.6889567375183105,\n",
       "  1.8821558952331543,\n",
       "  1.7139294147491455,\n",
       "  1.8269572257995605,\n",
       "  2.1100196838378906,\n",
       "  1.8686134815216064,\n",
       "  2.218608856201172,\n",
       "  2.088812828063965,\n",
       "  1.9797521829605103,\n",
       "  1.8329520225524902,\n",
       "  1.9106745719909668,\n",
       "  1.71274733543396,\n",
       "  1.7390873432159424,\n",
       "  1.5858874320983887,\n",
       "  1.7412660121917725,\n",
       "  1.4124109745025635,\n",
       "  1.858180046081543,\n",
       "  1.9182219505310059,\n",
       "  1.6848852634429932,\n",
       "  1.795989751815796],\n",
       " 14: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17311544716358185,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 15: [0.0,\n",
       "  0.02233952283859253,\n",
       "  0.0,\n",
       "  0.16375547647476196,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 16: [1.0445630550384521,\n",
       "  2.3224267959594727,\n",
       "  1.6898385286331177,\n",
       "  1.4141333103179932,\n",
       "  1.344003438949585,\n",
       "  2.5357654094696045,\n",
       "  1.0305070877075195,\n",
       "  1.5053768157958984,\n",
       "  1.1202802658081055,\n",
       "  1.433776617050171,\n",
       "  1.0935457944869995,\n",
       "  0.637639045715332,\n",
       "  1.6961145401000977,\n",
       "  0.8635381460189819,\n",
       "  1.280166506767273,\n",
       "  1.3994054794311523,\n",
       "  0.44759008288383484,\n",
       "  0.8642183542251587,\n",
       "  0.7990124225616455,\n",
       "  0.9695010185241699,\n",
       "  1.9079575538635254,\n",
       "  0.9336594343185425,\n",
       "  1.364335298538208,\n",
       "  2.136629581451416,\n",
       "  0.5815567970275879,\n",
       "  0.7751338481903076,\n",
       "  1.513148307800293,\n",
       "  1.1517952680587769,\n",
       "  1.483582615852356,\n",
       "  1.3502877950668335,\n",
       "  0.7807296514511108,\n",
       "  1.1168268918991089,\n",
       "  1.8070013523101807,\n",
       "  0.7295713424682617,\n",
       "  1.4779787063598633,\n",
       "  0.0,\n",
       "  1.7409968376159668,\n",
       "  1.2352840900421143,\n",
       "  2.354757308959961,\n",
       "  1.3636467456817627],\n",
       " 17: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 18: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08383464813232422,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 19: [2.726635217666626,\n",
       "  1.9143340587615967,\n",
       "  1.45002281665802,\n",
       "  1.5555534362792969,\n",
       "  1.7061512470245361,\n",
       "  2.1765635013580322,\n",
       "  2.258551597595215,\n",
       "  2.1047918796539307,\n",
       "  1.3812671899795532,\n",
       "  2.1706085205078125,\n",
       "  1.8533806800842285,\n",
       "  1.3054397106170654,\n",
       "  1.736396074295044,\n",
       "  1.9603207111358643,\n",
       "  1.8871397972106934,\n",
       "  2.6864917278289795,\n",
       "  2.1046457290649414,\n",
       "  2.029909610748291,\n",
       "  2.553044557571411,\n",
       "  1.7755942344665527,\n",
       "  0.5859454274177551,\n",
       "  2.0985188484191895,\n",
       "  1.563347339630127,\n",
       "  1.631269097328186,\n",
       "  1.7774072885513306,\n",
       "  1.625830054283142,\n",
       "  1.7058931589126587,\n",
       "  1.9238030910491943,\n",
       "  1.5909795761108398,\n",
       "  2.106604814529419,\n",
       "  1.8176968097686768,\n",
       "  1.2635951042175293,\n",
       "  1.675184726715088,\n",
       "  1.7334915399551392,\n",
       "  1.4445836544036865,\n",
       "  2.60507869720459,\n",
       "  1.3805798292160034,\n",
       "  1.4099910259246826,\n",
       "  1.8900442123413086,\n",
       "  1.6180620193481445],\n",
       " 20: [3.169858932495117,\n",
       "  3.363332509994507,\n",
       "  3.2805519104003906,\n",
       "  3.0371482372283936,\n",
       "  3.1069753170013428,\n",
       "  3.241806983947754,\n",
       "  3.2375192642211914,\n",
       "  3.5220367908477783,\n",
       "  3.3012912273406982,\n",
       "  2.9601728916168213,\n",
       "  3.167271375656128,\n",
       "  3.3607752323150635,\n",
       "  3.229618787765503,\n",
       "  3.322305917739868,\n",
       "  2.955443859100342,\n",
       "  3.564805746078491,\n",
       "  3.463292360305786,\n",
       "  3.3007404804229736,\n",
       "  3.3192965984344482,\n",
       "  2.89044451713562,\n",
       "  3.344048500061035,\n",
       "  3.365063190460205,\n",
       "  3.5838143825531006,\n",
       "  3.4823007583618164,\n",
       "  3.2850701808929443,\n",
       "  3.46256422996521,\n",
       "  2.837505340576172,\n",
       "  3.28214430809021,\n",
       "  3.558335065841675,\n",
       "  3.220792531967163,\n",
       "  3.774278402328491,\n",
       "  3.58180832862854,\n",
       "  3.182734966278076,\n",
       "  3.102276086807251,\n",
       "  3.2934648990631104,\n",
       "  3.039564609527588,\n",
       "  3.3696019649505615,\n",
       "  3.3541128635406494,\n",
       "  3.2615435123443604,\n",
       "  3.4697372913360596],\n",
       " 21: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 22: [4.441146373748779,\n",
       "  3.6933741569519043,\n",
       "  3.8284459114074707,\n",
       "  4.356447219848633,\n",
       "  3.591496467590332,\n",
       "  3.8801379203796387,\n",
       "  3.6615042686462402,\n",
       "  4.019923210144043,\n",
       "  3.871023654937744,\n",
       "  3.9804601669311523,\n",
       "  3.9304020404815674,\n",
       "  3.982208728790283,\n",
       "  4.582474231719971,\n",
       "  3.8192715644836426,\n",
       "  4.314121246337891,\n",
       "  3.7668004035949707,\n",
       "  4.043623447418213,\n",
       "  3.78584361076355,\n",
       "  4.684757232666016,\n",
       "  3.693944215774536,\n",
       "  4.035764217376709,\n",
       "  3.9375557899475098,\n",
       "  3.9051363468170166,\n",
       "  3.8692750930786133,\n",
       "  4.271055698394775,\n",
       "  4.355226516723633,\n",
       "  3.8000264167785645,\n",
       "  3.9286534786224365,\n",
       "  4.245790004730225,\n",
       "  3.971508502960205,\n",
       "  4.109339237213135,\n",
       "  4.1616740226745605,\n",
       "  3.570882797241211,\n",
       "  3.30381441116333,\n",
       "  4.161619186401367,\n",
       "  3.2696471214294434,\n",
       "  3.989307403564453,\n",
       "  3.4683001041412354,\n",
       "  4.2099289894104,\n",
       "  4.095142364501953],\n",
       " 23: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 24: [2.539989948272705,\n",
       "  2.931325912475586,\n",
       "  2.1550776958465576,\n",
       "  2.619936466217041,\n",
       "  2.587078332901001,\n",
       "  2.5354156494140625,\n",
       "  2.5751919746398926,\n",
       "  2.531525135040283,\n",
       "  2.77441668510437,\n",
       "  2.6561765670776367,\n",
       "  2.5015430450439453,\n",
       "  2.615279197692871,\n",
       "  2.64382266998291,\n",
       "  2.506624221801758,\n",
       "  2.2354488372802734,\n",
       "  2.709538698196411,\n",
       "  2.3817224502563477,\n",
       "  2.614347457885742,\n",
       "  2.363255262374878,\n",
       "  2.718179702758789,\n",
       "  2.9855475425720215,\n",
       "  2.6726138591766357,\n",
       "  2.6478896141052246,\n",
       "  2.6944704055786133,\n",
       "  3.047621488571167,\n",
       "  2.530758857727051,\n",
       "  2.3763504028320312,\n",
       "  2.3238601684570312,\n",
       "  2.335092067718506,\n",
       "  2.581996440887451,\n",
       "  2.5317001342773438,\n",
       "  2.8594448566436768,\n",
       "  2.8458046913146973,\n",
       "  2.4891767501831055,\n",
       "  2.7278356552124023,\n",
       "  2.530333995819092,\n",
       "  2.685863494873047,\n",
       "  2.532792091369629,\n",
       "  2.7236862182617188,\n",
       "  2.8422763347625732],\n",
       " 25: [3.08152174949646,\n",
       "  2.7795398235321045,\n",
       "  2.968859910964966,\n",
       "  2.932399272918701,\n",
       "  3.322330951690674,\n",
       "  3.2225348949432373,\n",
       "  3.30686616897583,\n",
       "  3.209494113922119,\n",
       "  3.0690886974334717,\n",
       "  3.371255874633789,\n",
       "  3.1232199668884277,\n",
       "  2.915087938308716,\n",
       "  3.0868403911590576,\n",
       "  3.6448519229888916,\n",
       "  3.0076184272766113,\n",
       "  2.82655930519104,\n",
       "  3.167297124862671,\n",
       "  3.258594036102295,\n",
       "  3.4921765327453613,\n",
       "  3.5772783756256104,\n",
       "  2.8427224159240723,\n",
       "  3.0043540000915527,\n",
       "  3.0630970001220703,\n",
       "  3.434993028640747,\n",
       "  2.9356818199157715,\n",
       "  2.9754130840301514,\n",
       "  3.0749685764312744,\n",
       "  2.9518468379974365,\n",
       "  3.1086957454681396,\n",
       "  3.071355104446411,\n",
       "  2.7352771759033203,\n",
       "  2.94895601272583,\n",
       "  3.1016316413879395,\n",
       "  3.290125846862793,\n",
       "  2.692079544067383,\n",
       "  2.999136209487915,\n",
       "  2.959946632385254,\n",
       "  3.307295799255371,\n",
       "  3.0565433502197266,\n",
       "  3.1745240688323975],\n",
       " 26: [0.2632741928100586,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.27126944065093994,\n",
       "  0.11380648612976074,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.10005736351013184,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.3612173795700073,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.020778894424438477,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1777505874633789,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0889003276824951,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07605016231536865,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 27: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 28: [2.9833269119262695,\n",
       "  3.0702316761016846,\n",
       "  2.8429479598999023,\n",
       "  3.4046707153320312,\n",
       "  2.9784886837005615,\n",
       "  3.004157781600952,\n",
       "  2.7042431831359863,\n",
       "  2.5729212760925293,\n",
       "  3.1312129497528076,\n",
       "  3.162435293197632,\n",
       "  3.3042848110198975,\n",
       "  3.1919641494750977,\n",
       "  3.059372901916504,\n",
       "  3.5550193786621094,\n",
       "  2.936887741088867,\n",
       "  3.2067718505859375,\n",
       "  2.937870740890503,\n",
       "  3.3818159103393555,\n",
       "  3.1562628746032715,\n",
       "  3.0696475505828857,\n",
       "  2.6778781414031982,\n",
       "  3.0360169410705566,\n",
       "  3.092834949493408,\n",
       "  3.0550308227539062,\n",
       "  3.2380356788635254,\n",
       "  3.319365978240967,\n",
       "  3.0896122455596924,\n",
       "  2.871220588684082,\n",
       "  2.9568848609924316,\n",
       "  2.9095983505249023,\n",
       "  2.954456090927124,\n",
       "  2.9713327884674072,\n",
       "  3.1585023403167725,\n",
       "  2.999605417251587,\n",
       "  2.999345064163208,\n",
       "  2.9697256088256836,\n",
       "  2.7113490104675293,\n",
       "  2.8723926544189453,\n",
       "  2.894242525100708,\n",
       "  2.5649335384368896],\n",
       " 29: [3.3173294067382812,\n",
       "  4.310837745666504,\n",
       "  3.265519618988037,\n",
       "  3.3872268199920654,\n",
       "  3.823179244995117,\n",
       "  3.400021553039551,\n",
       "  3.559804916381836,\n",
       "  2.8433737754821777,\n",
       "  3.440467596054077,\n",
       "  3.7546374797821045,\n",
       "  2.950098991394043,\n",
       "  2.796531915664673,\n",
       "  2.747044086456299,\n",
       "  4.0283613204956055,\n",
       "  3.2402381896972656,\n",
       "  3.0177793502807617,\n",
       "  3.8429880142211914,\n",
       "  4.063855171203613,\n",
       "  2.9182016849517822,\n",
       "  3.455733299255371,\n",
       "  3.503612995147705,\n",
       "  3.544503688812256,\n",
       "  3.8182272911071777,\n",
       "  2.9268200397491455,\n",
       "  2.991058349609375,\n",
       "  3.0749623775482178,\n",
       "  3.3087804317474365,\n",
       "  4.146646022796631,\n",
       "  4.058902740478516,\n",
       "  2.718771457672119,\n",
       "  3.677341938018799,\n",
       "  3.588811159133911,\n",
       "  3.4917075634002686,\n",
       "  3.1997921466827393,\n",
       "  3.5849499702453613,\n",
       "  3.2150931358337402,\n",
       "  3.313732385635376,\n",
       "  2.9896583557128906,\n",
       "  3.2033889293670654,\n",
       "  3.0348525047302246],\n",
       " 30: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 31: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 32: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 33: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 34: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9138612151145935,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08108991384506226,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 35: [3.3527374267578125,\n",
       "  3.5263350009918213,\n",
       "  3.3121848106384277,\n",
       "  3.3391542434692383,\n",
       "  3.125004529953003,\n",
       "  3.95753812789917,\n",
       "  3.343884229660034,\n",
       "  3.4793214797973633,\n",
       "  3.3721508979797363,\n",
       "  3.7474238872528076,\n",
       "  4.220724582672119,\n",
       "  3.638705015182495,\n",
       "  3.2935845851898193,\n",
       "  3.674694061279297,\n",
       "  3.5086586475372314,\n",
       "  3.512751579284668,\n",
       "  3.5878329277038574,\n",
       "  3.2954883575439453,\n",
       "  2.6073951721191406,\n",
       "  3.2975313663482666,\n",
       "  4.165125370025635,\n",
       "  3.823486804962158,\n",
       "  2.7370967864990234,\n",
       "  3.7387535572052,\n",
       "  3.4229066371917725,\n",
       "  2.963258743286133,\n",
       "  2.8752810955047607,\n",
       "  3.388096332550049,\n",
       "  3.3356916904449463,\n",
       "  3.4857826232910156,\n",
       "  3.423924446105957,\n",
       "  3.5044076442718506,\n",
       "  3.864236354827881,\n",
       "  3.296870708465576,\n",
       "  3.5110204219818115,\n",
       "  2.9859745502471924,\n",
       "  3.3492748737335205,\n",
       "  3.70359206199646,\n",
       "  3.0726003646850586,\n",
       "  3.4984495639801025],\n",
       " 36: [3.2852821350097656,\n",
       "  3.4468507766723633,\n",
       "  3.0232110023498535,\n",
       "  3.2561211585998535,\n",
       "  3.067929744720459,\n",
       "  3.725829601287842,\n",
       "  3.337106227874756,\n",
       "  3.243076801300049,\n",
       "  3.444882392883301,\n",
       "  3.688412666320801,\n",
       "  3.2510910034179688,\n",
       "  3.0510106086730957,\n",
       "  3.248809337615967,\n",
       "  3.265042304992676,\n",
       "  3.061891555786133,\n",
       "  3.4038357734680176,\n",
       "  3.1223883628845215,\n",
       "  2.6545472145080566,\n",
       "  3.550478458404541,\n",
       "  3.1886181831359863,\n",
       "  3.249950408935547,\n",
       "  2.9505348205566406,\n",
       "  3.173509120941162,\n",
       "  3.3468456268310547,\n",
       "  3.276608943939209,\n",
       "  3.653695583343506,\n",
       "  3.111758232116699,\n",
       "  3.2407946586608887,\n",
       "  3.3352785110473633,\n",
       "  3.476689338684082,\n",
       "  3.4700989723205566,\n",
       "  3.2261576652526855,\n",
       "  3.26572847366333,\n",
       "  3.7521581649780273,\n",
       "  2.8136138916015625,\n",
       "  3.3097610473632812,\n",
       "  2.9115347862243652,\n",
       "  3.2221503257751465,\n",
       "  3.3871026039123535,\n",
       "  3.3867950439453125],\n",
       " 37: [4.653278827667236,\n",
       "  4.6689934730529785,\n",
       "  4.827610492706299,\n",
       "  4.614624500274658,\n",
       "  4.859097003936768,\n",
       "  4.720193862915039,\n",
       "  4.967350959777832,\n",
       "  4.5034942626953125,\n",
       "  4.702145099639893,\n",
       "  4.533946514129639,\n",
       "  4.437066555023193,\n",
       "  4.400116920471191,\n",
       "  4.150121212005615,\n",
       "  3.914520502090454,\n",
       "  4.773404121398926,\n",
       "  4.5950541496276855,\n",
       "  4.770580768585205,\n",
       "  4.22641658782959,\n",
       "  5.1098151206970215,\n",
       "  4.308069705963135,\n",
       "  4.297675609588623,\n",
       "  5.677239894866943,\n",
       "  4.696278095245361,\n",
       "  4.064953804016113,\n",
       "  5.53479528427124,\n",
       "  4.754015922546387,\n",
       "  3.4064412117004395,\n",
       "  4.999006271362305,\n",
       "  5.24578332901001,\n",
       "  5.778918266296387,\n",
       "  4.642885208129883,\n",
       "  4.66196870803833,\n",
       "  5.04583215713501,\n",
       "  5.777281284332275,\n",
       "  5.466158390045166,\n",
       "  4.807226657867432,\n",
       "  5.166140079498291,\n",
       "  4.672362327575684,\n",
       "  4.866469860076904,\n",
       "  4.790965557098389],\n",
       " 38: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 39: [0.0,\n",
       "  0.17647847533226013,\n",
       "  0.20853650569915771,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.24942097067832947,\n",
       "  0.13154055178165436,\n",
       "  0.04044996201992035,\n",
       "  0.02276696264743805,\n",
       "  0.6469923853874207,\n",
       "  0.07673248648643494,\n",
       "  0.0,\n",
       "  0.008391931653022766,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2022678703069687,\n",
       "  0.14557307958602905,\n",
       "  0.03629954159259796,\n",
       "  0.011889621615409851,\n",
       "  0.3487153947353363,\n",
       "  0.1506030261516571,\n",
       "  0.0,\n",
       "  0.07834342122077942,\n",
       "  0.220409095287323,\n",
       "  0.10464015603065491,\n",
       "  0.0,\n",
       "  0.28348827362060547,\n",
       "  0.019269302487373352,\n",
       "  0.0,\n",
       "  0.29851609468460083,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0014705066569149494,\n",
       "  0.0,\n",
       "  0.1171654462814331,\n",
       "  0.12140189111232758,\n",
       "  0.25246718525886536,\n",
       "  0.0,\n",
       "  0.2364441454410553,\n",
       "  0.06208893656730652],\n",
       " 40: [0.0,\n",
       "  0.9263888597488403,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 41: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 42: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 43: [1.947451114654541,\n",
       "  2.260106086730957,\n",
       "  1.6782418489456177,\n",
       "  1.8466912508010864,\n",
       "  1.831846833229065,\n",
       "  2.0659499168395996,\n",
       "  1.7363437414169312,\n",
       "  1.9414740800857544,\n",
       "  2.1280455589294434,\n",
       "  2.0140175819396973,\n",
       "  2.0946290493011475,\n",
       "  1.601955771446228,\n",
       "  1.9767210483551025,\n",
       "  2.258877754211426,\n",
       "  2.057950496673584,\n",
       "  1.8378410339355469,\n",
       "  2.01865816116333,\n",
       "  1.6784898042678833,\n",
       "  1.7203108072280884,\n",
       "  1.627369999885559,\n",
       "  1.6714445352554321,\n",
       "  1.7494598627090454,\n",
       "  1.8896490335464478,\n",
       "  1.719958782196045,\n",
       "  2.783946990966797,\n",
       "  2.191530227661133,\n",
       "  1.7592483758926392,\n",
       "  1.8576749563217163,\n",
       "  1.808571219444275,\n",
       "  1.7980678081512451,\n",
       "  1.7005473375320435,\n",
       "  1.929012656211853,\n",
       "  1.9129246473312378,\n",
       "  1.7997206449508667,\n",
       "  1.942346453666687,\n",
       "  2.0321226119995117,\n",
       "  1.8639010190963745,\n",
       "  1.798683762550354,\n",
       "  2.050339698791504,\n",
       "  1.7411291599273682],\n",
       " 44: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 45: [3.9616763591766357,\n",
       "  4.464847564697266,\n",
       "  3.6014153957366943,\n",
       "  4.048863410949707,\n",
       "  3.2527546882629395,\n",
       "  3.6561317443847656,\n",
       "  3.6334922313690186,\n",
       "  3.6182491779327393,\n",
       "  3.8565359115600586,\n",
       "  3.575662612915039,\n",
       "  3.231611967086792,\n",
       "  3.5604639053344727,\n",
       "  3.4586400985717773,\n",
       "  3.257427215576172,\n",
       "  3.5224056243896484,\n",
       "  3.7773969173431396,\n",
       "  3.646648406982422,\n",
       "  3.8407607078552246,\n",
       "  3.7363169193267822,\n",
       "  3.6001992225646973,\n",
       "  3.7405707836151123,\n",
       "  3.757183074951172,\n",
       "  3.3235483169555664,\n",
       "  3.5277020931243896,\n",
       "  3.7549643516540527,\n",
       "  3.5229573249816895,\n",
       "  3.5782251358032227,\n",
       "  3.3457884788513184,\n",
       "  3.9055886268615723,\n",
       "  3.6933517456054688,\n",
       "  3.4997668266296387,\n",
       "  3.3795199394226074,\n",
       "  3.717906951904297,\n",
       "  3.477483034133911,\n",
       "  3.3725228309631348,\n",
       "  3.5427181720733643,\n",
       "  3.1600124835968018,\n",
       "  3.5673646926879883,\n",
       "  3.4617085456848145,\n",
       "  3.539569854736328],\n",
       " 46: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 47: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 48: [3.097501754760742,\n",
       "  3.601919412612915,\n",
       "  3.5378530025482178,\n",
       "  3.4535412788391113,\n",
       "  3.5413169860839844,\n",
       "  3.567594528198242,\n",
       "  3.4232378005981445,\n",
       "  3.534374952316284,\n",
       "  3.449066638946533,\n",
       "  3.468655824661255,\n",
       "  3.4177892208099365,\n",
       "  3.4138667583465576,\n",
       "  3.442391872406006,\n",
       "  3.552459478378296,\n",
       "  3.6319334506988525,\n",
       "  3.260152578353882,\n",
       "  3.4597742557525635,\n",
       "  3.73679256439209,\n",
       "  3.4246461391448975,\n",
       "  3.4954092502593994,\n",
       "  3.7273876667022705,\n",
       "  3.1243789196014404,\n",
       "  3.460578441619873,\n",
       "  3.385815143585205,\n",
       "  3.4450719356536865,\n",
       "  3.7213845252990723,\n",
       "  3.653999090194702,\n",
       "  3.402723789215088,\n",
       "  3.3508927822113037,\n",
       "  3.5015738010406494,\n",
       "  3.374833345413208,\n",
       "  3.531083106994629,\n",
       "  3.771616220474243,\n",
       "  3.4093987941741943,\n",
       "  3.7546143531799316,\n",
       "  3.477231025695801,\n",
       "  3.288757085800171,\n",
       "  3.5121476650238037,\n",
       "  3.363029718399048,\n",
       "  3.646916627883911],\n",
       " 49: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8240389823913574,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 50: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 51: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 52: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 53: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 54: [9.483621597290039,\n",
       "  10.29118824005127,\n",
       "  8.803369522094727,\n",
       "  8.264543533325195,\n",
       "  9.59683895111084,\n",
       "  9.110424041748047,\n",
       "  9.08452320098877,\n",
       "  8.739049911499023,\n",
       "  7.904854774475098,\n",
       "  7.648194789886475,\n",
       "  8.375933647155762,\n",
       "  9.47815227508545,\n",
       "  8.77721881866455,\n",
       "  9.104104042053223,\n",
       "  8.837175369262695,\n",
       "  9.230804443359375,\n",
       "  8.949234962463379,\n",
       "  9.01708698272705,\n",
       "  8.699419021606445,\n",
       "  8.817594528198242,\n",
       "  10.472552299499512,\n",
       "  9.24270248413086,\n",
       "  9.056187629699707,\n",
       "  9.9107666015625,\n",
       "  9.97897720336914,\n",
       "  9.384172439575195,\n",
       "  9.299903869628906,\n",
       "  8.162799835205078,\n",
       "  8.910665512084961,\n",
       "  9.85078239440918,\n",
       "  8.983280181884766,\n",
       "  9.050192832946777,\n",
       "  8.811420440673828,\n",
       "  9.40297794342041,\n",
       "  8.766956329345703,\n",
       "  9.430120468139648,\n",
       "  9.636906623840332,\n",
       "  8.352078437805176,\n",
       "  8.85687255859375,\n",
       "  8.418001174926758],\n",
       " 55: [7.22152853012085,\n",
       "  6.093258380889893,\n",
       "  7.062999248504639,\n",
       "  6.762032508850098,\n",
       "  6.643686771392822,\n",
       "  6.40079927444458,\n",
       "  7.100571155548096,\n",
       "  6.753149509429932,\n",
       "  8.166449546813965,\n",
       "  6.851559162139893,\n",
       "  6.982950210571289,\n",
       "  6.827667713165283,\n",
       "  7.11149263381958,\n",
       "  6.80546236038208,\n",
       "  6.491315841674805,\n",
       "  7.421327590942383,\n",
       "  6.923303127288818,\n",
       "  6.854676723480225,\n",
       "  7.374787330627441,\n",
       "  7.057592868804932,\n",
       "  5.765039920806885,\n",
       "  8.03875732421875,\n",
       "  7.350930690765381,\n",
       "  7.081897258758545,\n",
       "  6.68105936050415,\n",
       "  6.303632736206055,\n",
       "  7.050382137298584,\n",
       "  6.856308937072754,\n",
       "  7.04114294052124,\n",
       "  7.0801239013671875,\n",
       "  6.875813007354736,\n",
       "  7.379705905914307,\n",
       "  7.365818977355957,\n",
       "  6.877207279205322,\n",
       "  7.779823303222656,\n",
       "  6.084456443786621,\n",
       "  6.816384792327881,\n",
       "  5.373393535614014,\n",
       "  7.606034278869629,\n",
       "  7.6855316162109375],\n",
       " 56: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 57: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 58: [6.790973663330078,\n",
       "  7.237905502319336,\n",
       "  7.386713027954102,\n",
       "  8.561385154724121,\n",
       "  7.3344879150390625,\n",
       "  7.243391036987305,\n",
       "  7.300078392028809,\n",
       "  7.991931915283203,\n",
       "  6.725744724273682,\n",
       "  8.056041717529297,\n",
       "  7.436562538146973,\n",
       "  7.629868507385254,\n",
       "  6.4627580642700195,\n",
       "  7.58662223815918,\n",
       "  7.162380218505859,\n",
       "  7.657076835632324,\n",
       "  7.299384117126465,\n",
       "  7.487439155578613,\n",
       "  7.2583441734313965,\n",
       "  8.079222679138184,\n",
       "  7.817923545837402,\n",
       "  8.229459762573242,\n",
       "  6.831817150115967,\n",
       "  7.113531112670898,\n",
       "  7.476574420928955,\n",
       "  7.182344436645508,\n",
       "  7.734486103057861,\n",
       "  7.109537124633789,\n",
       "  6.518916130065918,\n",
       "  7.243256568908691,\n",
       "  7.874050617218018,\n",
       "  7.421475410461426,\n",
       "  8.582812309265137,\n",
       "  6.31135368347168,\n",
       "  7.4616217613220215,\n",
       "  7.707437515258789,\n",
       "  7.8331451416015625,\n",
       "  7.214650630950928,\n",
       "  6.89049768447876,\n",
       "  7.708538055419922],\n",
       " 59: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states={}\n",
    "for i in range(3):\n",
    "  states[i]={}\n",
    "\n",
    "with torch.no_grad():\n",
    "  for neuron in range(60):\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "      for i,t in zip(inputs,labels):\n",
    "        t=t.item()\n",
    "        outputs = model(i.unsqueeze(0))\n",
    "        out=activations[f'relu{(neuron//20)+1}'][:,neuron%20].tolist()[0]\n",
    "        if not neuron in states[t]:\n",
    "          states[t][neuron]=[]\n",
    "        states[t][neuron].append(out)\n",
    "\n",
    "with open(\"relu_states\", \"w\") as fp:\n",
    "  json.dump(states, fp)\n",
    "\n",
    "states[0]\n",
    "\n",
    "# states\n",
    "# json\n",
    "# label:{{neuron: states}, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeGQse17-vrY"
   },
   "source": [
    "###### initialize marabou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U66qhU6wLW55"
   },
   "outputs": [],
   "source": [
    "#start with all relu，删减直到不能verify？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hiBON9ZMEL-"
   },
   "outputs": [],
   "source": [
    "# network=init_network()\n",
    "# network.outputVars #4,5,6\n",
    "# network.inputVars #0,1,2,3\n",
    "# network.reluList #(i+10) for i in range(7,37)\n",
    "# network.numVars #47\n",
    "# offset=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.9129357933998108,\n",
       "  0.9532325267791748,\n",
       "  0.6676362156867981,\n",
       "  0.6903653144836426,\n",
       "  0.8410075306892395,\n",
       "  1.0494059324264526,\n",
       "  0.9137061238288879,\n",
       "  0.45926231145858765,\n",
       "  1.1056742668151855,\n",
       "  0.9550075531005859,\n",
       "  0.8734093308448792,\n",
       "  0.6723171472549438,\n",
       "  1.4888828992843628,\n",
       "  1.1625556945800781,\n",
       "  0.5503205060958862,\n",
       "  0.6151769161224365,\n",
       "  1.1982731819152832,\n",
       "  0.828767716884613,\n",
       "  0.750876784324646,\n",
       "  0.7998632788658142,\n",
       "  0.0,\n",
       "  0.6580435037612915,\n",
       "  0.7377094626426697,\n",
       "  1.2010773420333862,\n",
       "  1.2464121580123901,\n",
       "  0.8751844167709351,\n",
       "  1.0747358798980713,\n",
       "  1.5152419805526733,\n",
       "  1.2062480449676514,\n",
       "  0.6370339393615723,\n",
       "  1.649401307106018,\n",
       "  1.246646523475647,\n",
       "  0.7823510766029358,\n",
       "  0.40757331252098083,\n",
       "  1.0450611114501953,\n",
       "  0.7333645820617676,\n",
       "  0.5013340711593628,\n",
       "  0.8200780153274536,\n",
       "  0.9167447090148926,\n",
       "  1.0205014944076538],\n",
       " 1: [1.1909759044647217,\n",
       "  1.4731836318969727,\n",
       "  1.5156525373458862,\n",
       "  1.652188777923584,\n",
       "  0.929201602935791,\n",
       "  1.940536379814148,\n",
       "  1.3605681657791138,\n",
       "  1.1708238124847412,\n",
       "  1.2068039178848267,\n",
       "  1.1619681119918823,\n",
       "  1.060368299484253,\n",
       "  0.937010645866394,\n",
       "  0.9794766902923584,\n",
       "  0.68492591381073,\n",
       "  1.143419861793518,\n",
       "  0.9455838799476624,\n",
       "  1.3608477115631104,\n",
       "  0.8093271255493164,\n",
       "  1.8072762489318848,\n",
       "  1.4941742420196533,\n",
       "  1.148789405822754,\n",
       "  0.6534786820411682,\n",
       "  1.1880488395690918,\n",
       "  1.266777515411377,\n",
       "  1.727431297302246,\n",
       "  1.4250715970993042,\n",
       "  1.521022081375122,\n",
       "  1.1248717308044434,\n",
       "  0.8835259675979614,\n",
       "  1.7220617532730103,\n",
       "  1.4917347431182861,\n",
       "  1.2473864555358887,\n",
       "  1.2796765565872192,\n",
       "  1.2538753747940063,\n",
       "  1.2022740840911865,\n",
       "  1.0987879037857056,\n",
       "  1.312453031539917,\n",
       "  0.8233455419540405,\n",
       "  0.5333245992660522,\n",
       "  0.927594780921936],\n",
       " 2: [0.0,\n",
       "  0.37415599822998047,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2269478440284729,\n",
       "  0.014735877513885498,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.04803657531738281,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6656107306480408,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4165295958518982,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1212189793586731,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.12910127639770508,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 3: [2.559872627258301,\n",
       "  1.637089729309082,\n",
       "  2.3574485778808594,\n",
       "  2.1277005672454834,\n",
       "  2.2558200359344482,\n",
       "  2.337719678878784,\n",
       "  1.9011695384979248,\n",
       "  2.2411742210388184,\n",
       "  1.9840004444122314,\n",
       "  2.0225274562835693,\n",
       "  1.8583076000213623,\n",
       "  1.9189352989196777,\n",
       "  2.1264426708221436,\n",
       "  1.8792946338653564,\n",
       "  1.985581398010254,\n",
       "  1.1012401580810547,\n",
       "  2.5859429836273193,\n",
       "  1.9593284130096436,\n",
       "  2.0632436275482178,\n",
       "  2.049065351486206,\n",
       "  1.9144599437713623,\n",
       "  1.7103147506713867,\n",
       "  2.067718744277954,\n",
       "  2.265199899673462,\n",
       "  2.012356996536255,\n",
       "  1.8635733127593994,\n",
       "  1.9039664268493652,\n",
       "  2.217254638671875,\n",
       "  2.153162956237793,\n",
       "  2.0887060165405273,\n",
       "  2.0325534343719482,\n",
       "  1.6302809715270996,\n",
       "  2.6628525257110596,\n",
       "  2.6864962577819824,\n",
       "  2.247192144393921,\n",
       "  1.4919910430908203,\n",
       "  2.118605375289917,\n",
       "  2.1762535572052,\n",
       "  1.609153389930725,\n",
       "  2.1913628578186035],\n",
       " 4: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 5: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.05117669701576233,\n",
       "  0.4270865321159363,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.16383260488510132,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.24622851610183716,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 6: [0.1500973105430603,\n",
       "  0.15429240465164185,\n",
       "  0.21747636795043945,\n",
       "  0.08104681968688965,\n",
       "  0.30280500650405884,\n",
       "  0.23485684394836426,\n",
       "  0.0,\n",
       "  0.24652260541915894,\n",
       "  0.2947728633880615,\n",
       "  0.25481945276260376,\n",
       "  0.23294800519943237,\n",
       "  0.23260116577148438,\n",
       "  0.030681908130645752,\n",
       "  0.31634825468063354,\n",
       "  0.0,\n",
       "  0.02391570806503296,\n",
       "  0.08150643110275269,\n",
       "  0.0,\n",
       "  0.06007498502731323,\n",
       "  0.03785675764083862,\n",
       "  0.0,\n",
       "  0.07286274433135986,\n",
       "  0.06568795442581177,\n",
       "  0.28331297636032104,\n",
       "  0.10080665349960327,\n",
       "  0.01931208372116089,\n",
       "  0.0,\n",
       "  0.11761480569839478,\n",
       "  0.12721675634384155,\n",
       "  0.1656622290611267,\n",
       "  0.08902806043624878,\n",
       "  0.0,\n",
       "  0.21263855695724487,\n",
       "  0.2773532271385193,\n",
       "  0.0,\n",
       "  0.26598337292671204,\n",
       "  0.003606259822845459,\n",
       "  0.10812246799468994,\n",
       "  0.07089215517044067,\n",
       "  0.07210719585418701],\n",
       " 7: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 8: [1.0014560222625732,\n",
       "  1.9579131603240967,\n",
       "  0.0,\n",
       "  0.9110982418060303,\n",
       "  1.2812013626098633,\n",
       "  1.3394477367401123,\n",
       "  1.003240704536438,\n",
       "  0.7876605987548828,\n",
       "  1.070023775100708,\n",
       "  1.7273995876312256,\n",
       "  1.2047357559204102,\n",
       "  0.9601218700408936,\n",
       "  1.058854103088379,\n",
       "  0.47373542189598083,\n",
       "  0.8476023077964783,\n",
       "  0.6091015338897705,\n",
       "  0.2790970206260681,\n",
       "  1.5100195407867432,\n",
       "  0.7536750435829163,\n",
       "  1.9850571155548096,\n",
       "  1.686065435409546,\n",
       "  1.0612928867340088,\n",
       "  2.2798101902008057,\n",
       "  1.1695146560668945,\n",
       "  1.0595080852508545,\n",
       "  1.4933013916015625,\n",
       "  1.6050927639007568,\n",
       "  0.2234683632850647,\n",
       "  1.534635305404663,\n",
       "  1.3992846012115479,\n",
       "  1.04899263381958,\n",
       "  1.335668683052063,\n",
       "  1.3580553531646729,\n",
       "  1.273303508758545,\n",
       "  0.7234058380126953,\n",
       "  1.1404809951782227,\n",
       "  0.8036351799964905,\n",
       "  0.448925256729126,\n",
       "  1.8769402503967285,\n",
       "  1.5618691444396973],\n",
       " 9: [1.094372272491455,\n",
       "  0.2282760739326477,\n",
       "  1.0509209632873535,\n",
       "  0.6216519474983215,\n",
       "  0.4878440201282501,\n",
       "  0.9950289726257324,\n",
       "  0.5277879238128662,\n",
       "  0.47467872500419617,\n",
       "  0.6775438189506531,\n",
       "  1.1882362365722656,\n",
       "  1.4677493572235107,\n",
       "  0.0,\n",
       "  0.6285180449485779,\n",
       "  0.5554190278053284,\n",
       "  0.2607153654098511,\n",
       "  0.6271311044692993,\n",
       "  0.8038471341133118,\n",
       "  0.8742585182189941,\n",
       "  0.18342888355255127,\n",
       "  1.1916900873184204,\n",
       "  0.5367746353149414,\n",
       "  1.2027021646499634,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4518972933292389,\n",
       "  0.15992268919944763,\n",
       "  1.3269801139831543,\n",
       "  0.2317299246788025,\n",
       "  0.33178895711898804,\n",
       "  0.0,\n",
       "  0.46569210290908813,\n",
       "  1.0923469066619873,\n",
       "  1.383596658706665,\n",
       "  0.7099831104278564,\n",
       "  0.8686721324920654,\n",
       "  0.7983142733573914,\n",
       "  0.7452051639556885,\n",
       "  0.0,\n",
       "  2.0446746349334717,\n",
       "  0.0],\n",
       " 10: [3.169858932495117,\n",
       "  3.46256422996521,\n",
       "  3.774278402328491,\n",
       "  2.955443859100342,\n",
       "  3.3012912273406982,\n",
       "  3.322305917739868,\n",
       "  3.182734966278076,\n",
       "  3.5220367908477783,\n",
       "  3.4823007583618164,\n",
       "  3.167271375656128,\n",
       "  3.58180832862854,\n",
       "  3.564805746078491,\n",
       "  3.558335065841675,\n",
       "  3.2615435123443604,\n",
       "  3.365063190460205,\n",
       "  3.0371482372283936,\n",
       "  3.4697372913360596,\n",
       "  3.3007404804229736,\n",
       "  3.229618787765503,\n",
       "  3.3696019649505615,\n",
       "  3.220792531967163,\n",
       "  3.344048500061035,\n",
       "  3.039564609527588,\n",
       "  2.89044451713562,\n",
       "  3.1069753170013428,\n",
       "  3.241806983947754,\n",
       "  3.28214430809021,\n",
       "  3.2375192642211914,\n",
       "  2.9601728916168213,\n",
       "  3.3541128635406494,\n",
       "  2.837505340576172,\n",
       "  3.3192965984344482,\n",
       "  3.2805519104003906,\n",
       "  3.2934648990631104,\n",
       "  3.2850701808929443,\n",
       "  3.102276086807251,\n",
       "  3.463292360305786,\n",
       "  3.5838143825531006,\n",
       "  3.3607752323150635,\n",
       "  3.363332509994507],\n",
       " 11: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 12: [3.8284459114074707,\n",
       "  3.8000264167785645,\n",
       "  3.971508502960205,\n",
       "  4.2099289894104,\n",
       "  3.982208728790283,\n",
       "  4.043623447418213,\n",
       "  4.109339237213135,\n",
       "  3.591496467590332,\n",
       "  4.095142364501953,\n",
       "  4.355226516723633,\n",
       "  3.570882797241211,\n",
       "  4.582474231719971,\n",
       "  3.989307403564453,\n",
       "  3.8192715644836426,\n",
       "  4.314121246337891,\n",
       "  3.9304020404815674,\n",
       "  4.684757232666016,\n",
       "  3.4683001041412354,\n",
       "  4.245790004730225,\n",
       "  4.161619186401367,\n",
       "  4.019923210144043,\n",
       "  3.2696471214294434,\n",
       "  4.1616740226745605,\n",
       "  3.871023654937744,\n",
       "  3.8801379203796387,\n",
       "  4.035764217376709,\n",
       "  3.30381441116333,\n",
       "  4.441146373748779,\n",
       "  3.9375557899475098,\n",
       "  3.8692750930786133,\n",
       "  3.9286534786224365,\n",
       "  3.9804601669311523,\n",
       "  3.7668004035949707,\n",
       "  4.271055698394775,\n",
       "  4.356447219848633,\n",
       "  3.6933741569519043,\n",
       "  3.693944215774536,\n",
       "  3.6615042686462402,\n",
       "  3.78584361076355,\n",
       "  3.9051363468170166],\n",
       " 13: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 14: [2.5751919746398926,\n",
       "  2.506624221801758,\n",
       "  2.3817224502563477,\n",
       "  2.709538698196411,\n",
       "  2.5354156494140625,\n",
       "  2.587078332901001,\n",
       "  2.5317001342773438,\n",
       "  2.363255262374878,\n",
       "  3.047621488571167,\n",
       "  2.8458046913146973,\n",
       "  2.3238601684570312,\n",
       "  2.3763504028320312,\n",
       "  2.8422763347625732,\n",
       "  2.619936466217041,\n",
       "  2.7278356552124023,\n",
       "  2.6478896141052246,\n",
       "  2.7236862182617188,\n",
       "  2.581996440887451,\n",
       "  2.614347457885742,\n",
       "  2.6726138591766357,\n",
       "  2.931325912475586,\n",
       "  2.8594448566436768,\n",
       "  2.335092067718506,\n",
       "  2.718179702758789,\n",
       "  2.532792091369629,\n",
       "  2.6561765670776367,\n",
       "  2.685863494873047,\n",
       "  2.4891767501831055,\n",
       "  2.539989948272705,\n",
       "  2.2354488372802734,\n",
       "  2.531525135040283,\n",
       "  2.5015430450439453,\n",
       "  2.6944704055786133,\n",
       "  2.615279197692871,\n",
       "  2.64382266998291,\n",
       "  2.77441668510437,\n",
       "  2.530333995819092,\n",
       "  2.1550776958465576,\n",
       "  2.9855475425720215,\n",
       "  2.530758857727051],\n",
       " 15: [2.8427224159240723,\n",
       "  2.692079544067383,\n",
       "  2.915087938308716,\n",
       "  3.307295799255371,\n",
       "  3.0749685764312744,\n",
       "  3.290125846862793,\n",
       "  3.30686616897583,\n",
       "  3.1086957454681396,\n",
       "  2.82655930519104,\n",
       "  2.999136209487915,\n",
       "  3.167297124862671,\n",
       "  2.9356818199157715,\n",
       "  2.932399272918701,\n",
       "  3.0076184272766113,\n",
       "  2.7352771759033203,\n",
       "  3.258594036102295,\n",
       "  3.0868403911590576,\n",
       "  2.959946632385254,\n",
       "  3.0690886974334717,\n",
       "  3.1745240688323975,\n",
       "  3.6448519229888916,\n",
       "  2.9754130840301514,\n",
       "  2.94895601272583,\n",
       "  3.071355104446411,\n",
       "  3.209494113922119,\n",
       "  3.0565433502197266,\n",
       "  3.322330951690674,\n",
       "  2.968859910964966,\n",
       "  3.4921765327453613,\n",
       "  3.434993028640747,\n",
       "  2.7795398235321045,\n",
       "  3.1232199668884277,\n",
       "  3.0043540000915527,\n",
       "  3.2225348949432373,\n",
       "  3.371255874633789,\n",
       "  3.0630970001220703,\n",
       "  3.1016316413879395,\n",
       "  3.08152174949646,\n",
       "  2.9518468379974365,\n",
       "  3.5772783756256104],\n",
       " 16: [0.10005736351013184,\n",
       "  0.0,\n",
       "  0.07605016231536865,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2632741928100586,\n",
       "  0.0,\n",
       "  0.020778894424438477,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1777505874633789,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0889003276824951,\n",
       "  0.3612173795700073,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.27126944065093994,\n",
       "  0.0,\n",
       "  0.11380648612976074,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 17: [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 18: [2.7042431831359863,\n",
       "  3.3818159103393555,\n",
       "  2.954456090927124,\n",
       "  2.5649335384368896,\n",
       "  2.7113490104675293,\n",
       "  3.1562628746032715,\n",
       "  2.6778781414031982,\n",
       "  3.319365978240967,\n",
       "  3.004157781600952,\n",
       "  2.8723926544189453,\n",
       "  3.4046707153320312,\n",
       "  3.0360169410705566,\n",
       "  3.092834949493408,\n",
       "  2.894242525100708,\n",
       "  3.1585023403167725,\n",
       "  2.999605417251587,\n",
       "  3.0896122455596924,\n",
       "  3.3042848110198975,\n",
       "  2.9833269119262695,\n",
       "  3.0702316761016846,\n",
       "  3.5550193786621094,\n",
       "  3.2380356788635254,\n",
       "  2.9713327884674072,\n",
       "  2.8429479598999023,\n",
       "  2.9784886837005615,\n",
       "  2.9095983505249023,\n",
       "  3.1919641494750977,\n",
       "  3.2067718505859375,\n",
       "  3.162435293197632,\n",
       "  2.9568848609924316,\n",
       "  2.937870740890503,\n",
       "  2.999345064163208,\n",
       "  2.871220588684082,\n",
       "  3.0696475505828857,\n",
       "  3.059372901916504,\n",
       "  2.5729212760925293,\n",
       "  2.9697256088256836,\n",
       "  3.0550308227539062,\n",
       "  3.1312129497528076,\n",
       "  2.936887741088867],\n",
       " 19: [3.588811159133911,\n",
       "  3.8429880142211914,\n",
       "  2.718771457672119,\n",
       "  3.400021553039551,\n",
       "  3.503612995147705,\n",
       "  3.455733299255371,\n",
       "  4.146646022796631,\n",
       "  3.0348525047302246,\n",
       "  3.2150931358337402,\n",
       "  4.058902740478516,\n",
       "  2.950098991394043,\n",
       "  2.8433737754821777,\n",
       "  3.0177793502807617,\n",
       "  3.8182272911071777,\n",
       "  3.1997921466827393,\n",
       "  2.9182016849517822,\n",
       "  2.747044086456299,\n",
       "  3.440467596054077,\n",
       "  3.2402381896972656,\n",
       "  3.265519618988037,\n",
       "  3.5849499702453613,\n",
       "  3.313732385635376,\n",
       "  3.544503688812256,\n",
       "  2.991058349609375,\n",
       "  3.823179244995117,\n",
       "  2.9268200397491455,\n",
       "  3.0749623775482178,\n",
       "  2.9896583557128906,\n",
       "  3.559804916381836,\n",
       "  3.3087804317474365,\n",
       "  3.3872268199920654,\n",
       "  3.7546374797821045,\n",
       "  3.677341938018799,\n",
       "  3.4917075634002686,\n",
       "  4.310837745666504,\n",
       "  2.796531915664673,\n",
       "  3.3173294067382812,\n",
       "  4.0283613204956055,\n",
       "  3.2033889293670654,\n",
       "  4.063855171203613]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STABLE_PATTERNS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6i9vizIqo4EV",
    "outputId": "8a11138f-07f1-44e8-86c6-24e577cd9c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = init_network()\n",
    "label=2\n",
    "other_label=1\n",
    "label_states=STABLE_PATTERNS[label]\n",
    "network = add_relu_constraints(network, label_states) #previously commented out??\n",
    "offset = network.outputVars[0][0][0]\n",
    "for i in range(4):\n",
    "  network.setLowerBound(i, 0)\n",
    "  network.setUpperBound(i, 1)\n",
    "\n",
    "for neuron,values in label_states.items():\n",
    "        layer, idx, marabou_idx = parse_raw_idx(neuron)\n",
    "        #print(f'{neuron}: {values}')\n",
    "        if values.count(0) == len(values): #len(X_train)*0.5:\n",
    "            constraint = MarabouUtils.Equation(MarabouCore.Equation.LE)\n",
    "            constraint.addAddend(1, marabou_idx)\n",
    "            constraint.setScalar(-0.001)\n",
    "        else:\n",
    "            constraint = MarabouUtils.Equation(MarabouCore.Equation.GE)\n",
    "            constraint.addAddend(1, marabou_idx)\n",
    "            constraint.setScalar(0.001)\n",
    "        network.addEquation(constraint)\n",
    "        # import pdb;pdb.set_trace()\n",
    "\n",
    "#add output constraint\n",
    "constraint = MarabouUtils.Equation(MarabouCore.Equation.GE)\n",
    "constraint.addAddend(1, other_label+offset)\n",
    "constraint.addAddend(-1, label+offset)\n",
    "constraint.setScalar(0.001)\n",
    "network.addEquation(constraint)\n",
    "\n",
    "\n",
    "#add additional bounds here\n",
    "exit_code: str\n",
    "exit_code, vals, stats = network.solve(options=M_OPTIONS)\n",
    "len(network.equList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Sgzg-rxS2TO",
    "outputId": "e31dcf06-30d3-4884-91e6-39c5e1f7d00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For label 0, check if its stable RELU pattern guarantees the output\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "For label 1, check if its stable RELU pattern guarantees the output\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "For label 2, check if its stable RELU pattern guarantees the output\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "              0             1             2\n",
      "0          -1.0   UNS:0.00186  UNS:0.000906\n",
      "1  UNS:0.001954          -1.0  UNS:0.001634\n",
      "2  UNS:0.001045  UNS:0.001034          -1.0\n"
     ]
    }
   ],
   "source": [
    "def add_relu_constraints(network: Marabou.MarabouNetworkNNet, label_states)->Marabou.MarabouNetworkNNet:\n",
    "    \"\"\"\n",
    "    Add stable relus constraints to the Marabou network\n",
    "    \"\"\"\n",
    "    for neuron,values in label_states.items():\n",
    "        layer, idx, marabou_idx = parse_raw_idx(neuron)\n",
    "        #print(f'{neuron}: {values}')\n",
    "        if values.count(0) > len(values)*0.5: #len(X_train)*0.5:  len(values)\n",
    "            constraint = MarabouUtils.Equation(MarabouCore.Equation.LE)\n",
    "            constraint.addAddend(1, marabou_idx)\n",
    "            constraint.setScalar(-0.001)\n",
    "        else:\n",
    "            constraint = MarabouUtils.Equation(MarabouCore.Equation.GE)\n",
    "            constraint.addAddend(1, marabou_idx)\n",
    "            constraint.setScalar(0.001)\n",
    "        network.addEquation(constraint)\n",
    "        # import pdb;pdb.set_trace()\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "\n",
    "def check_pattern(label_states, label: int, other_label: int)->Tuple[str, int]: #relu_check_list: List[int], relu_val: List[int]\n",
    "    \"\"\"\n",
    "    In ACAS, the prediction is the label with smallest value.\n",
    "    So we check that label - other_label < 0 forall input\n",
    "    by finding assignments for label - other_label >=0\n",
    "    \"\"\"\n",
    "    print(\"--------CHECK PATTERN: output_{} is always less than output_{} ? --------\".format(label, other_label))\n",
    "    network = init_network()\n",
    "    network = add_relu_constraints(network, label_states) #previously commented out??\n",
    "    offset = network.outputVars[0][0][0]\n",
    "    for i in range(4):\n",
    "      network.setLowerBound(i, 0)\n",
    "      network.setUpperBound(i, 1)\n",
    "    #add output constraint\n",
    "    constraint = MarabouUtils.Equation(MarabouCore.Equation.GE)\n",
    "    constraint.addAddend(1, other_label+offset)\n",
    "    constraint.addAddend(-1, label+offset)\n",
    "    constraint.setScalar(0.001)\n",
    "    network.addEquation(constraint)\n",
    "\n",
    "\n",
    "    #add additional bounds here\n",
    "    exit_code: str\n",
    "    exit_code, vals, stats = network.solve(options=M_OPTIONS)\n",
    "\n",
    "    running_time:int = stats.getTotalTimeInMicro()\n",
    "\n",
    "    return exit_code, running_time\n",
    "\n",
    "def main():\n",
    "    res = [[-1.]*3 for i in range(3)]\n",
    "    # print(res)\n",
    "    for label in range(3):\n",
    "        print(f\"For label {label}, check if its stable RELU pattern guarantees the output\")\n",
    "        for other_label in range(3):#range(10):\n",
    "            if other_label == int(label):\n",
    "                continue\n",
    "            # relu_check_list = STABLE_PATTERNS[label][\"stable_idx\"]\n",
    "            # relu_val = STABLE_PATTERNS[label][\"val\"]\n",
    "            exit_code, running_time = check_pattern(STABLE_PATTERNS[label], label=int(label), other_label = other_label)\n",
    "            if exit_code==\"sat\":\n",
    "                res[int(label)][other_label] = \"SAT:{}\".format(running_time/10**6)\n",
    "                break\n",
    "            elif exit_code==\"unsat\":\n",
    "                res[int(label)][other_label] = \"UNS:{}\".format(running_time/10**6)\n",
    "\n",
    "            else:\n",
    "                res[int(label)][other_label] = exit_code\n",
    "\n",
    "    res = pandas.DataFrame(res)\n",
    "    print(res)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=59\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=58\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=57\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=56\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=55\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=54\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=53\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=52\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=51\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=50\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=49\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=48\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=47\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=46\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=45\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=44\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=43\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=42\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=41\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=40\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=39\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=38\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=37\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=36\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=35\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=34\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=33\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=32\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=31\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=30\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=29\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=28\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=27\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=26\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=25\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=24\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=23\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=22\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=21\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsat\n",
      "popped neuron=20\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=19\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=18\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=17\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=16\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=15\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=14\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=13\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=12\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=11\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=10\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=9\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=8\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=7\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=6\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=5\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=4\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=3\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=2\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "sat\n",
      "input 0 = 0.0\n",
      "input 1 = 0.692003955996536\n",
      "input 2 = 0.0\n",
      "input 3 = 1.0\n",
      "output 0 = -1.7755799363575848\n",
      "output 1 = 2.5651231833967794\n",
      "output 2 = -1.2338043001847825\n",
      "for label 0: remaining neurons= dict_keys([0, 1])\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=59\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=58\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=57\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=56\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=55\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=54\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=53\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=52\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=51\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=50\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=49\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=48\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=47\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=46\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=45\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=44\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=43\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=42\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=41\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=40\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=39\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=38\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=37\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=36\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=35\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=34\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=33\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=32\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=31\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=30\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=29\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=28\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=27\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=26\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=25\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=24\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=23\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=22\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=21\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=20\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=19\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=18\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=17\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=16\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=15\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=14\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=13\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=12\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsat\n",
      "popped neuron=11\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=10\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_1 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_1 is always less than output_2 ? --------\n",
      "sat\n",
      "input 0 = 0.0411738121766786\n",
      "input 1 = 0.26259771820785754\n",
      "input 2 = 0.3552448865722231\n",
      "input 3 = 1.0\n",
      "output 0 = -3.468105789532308\n",
      "output 1 = 1.7845909887205424\n",
      "output 2 = 1.7855909887205432\n",
      "for label 1: remaining neurons= dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=59\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=58\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=57\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=56\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=55\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "popped neuron=54\n",
      "<class 'dict'>\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "sat\n",
      "input 0 = 0.420266966363066\n",
      "input 1 = 0.6416982024774769\n",
      "input 2 = 0.7527845299390868\n",
      "input 3 = 0.7620443848529598\n",
      "output 0 = -3.84734153458875\n",
      "output 1 = 1.9988369033198008\n",
      "output 2 = 1.9978369033198011\n",
      "for label 2: remaining neurons= dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for label in range(3):\n",
    "  relu_label=STABLE_PATTERNS[label].copy()\n",
    "  print(type(relu_label))\n",
    "  def func(relus):\n",
    "    for other_label in range(3):\n",
    "      exit_code, running_time = check_pattern(relus, label=int(label), other_label = other_label)\n",
    "      if exit_code==\"sat\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "  flag=func(relu_label)\n",
    "  while flag:\n",
    "    popped_key=sorted(relu_label.keys())[-1]\n",
    "    relu_label.pop(popped_key)\n",
    "    print(f\"popped neuron={popped_key}\")\n",
    "    print(type(relu_label))\n",
    "    flag=func(relu_label)\n",
    "  print(f\"for label {label}: remaining neurons= {relu_label.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------CHECK PATTERN: output_0 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_0 is always less than output_2 ? --------\n",
      "unsat\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     23\u001b[0m     relu_label\u001b[38;5;241m=\u001b[39mSTABLE_PATTERNS[label]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mscancoarsen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelu_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36mscancoarsen\u001b[0;34m(relu_label)\u001b[0m\n\u001b[1;32m     10\u001b[0m nap \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m relu_label_copy \u001b[38;5;241m=\u001b[39m relu_label\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keyss:\n\u001b[1;32m     13\u001b[0m     relu_label_copy\u001b[38;5;241m.\u001b[39mpop(k)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func(relu_label_copy):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "def func(relus):\n",
    "    for other_label in range(3):\n",
    "      exit_code, running_time = check_pattern(relus, label=int(label), other_label = other_label)\n",
    "      if exit_code==\"sat\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def scancoarsen(relu_label):\n",
    "    keyss = relu_label.keys()\n",
    "    nap = []\n",
    "    relu_label_copy = relu_label.copy()\n",
    "    for k in keyss:\n",
    "        relu_label_copy.pop(k)\n",
    "        if func(relu_label_copy):\n",
    "            relu_label.pop(k)\n",
    "            continue\n",
    "        else:\n",
    "            nap.append(k)\n",
    "            relu_label_copy = relu_label.copy()\n",
    "    return relu_label.keys()\n",
    "            \n",
    "for label in range(3):\n",
    "    relu_label=STABLE_PATTERNS[label].copy()\n",
    "    scancoarsen(relu_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consider neuron=0\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=1\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=2\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=3\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=4\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=5\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=6\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=7\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=8\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=9\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=10\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=11\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=12\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=13\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=14\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=15\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=16\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=17\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=18\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=19\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=20\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=21\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=22\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=23\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=24\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=25\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=26\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=27\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=28\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=29\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=30\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=31\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=32\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=33\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=34\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=35\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=36\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=37\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=38\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=39\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=40\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=41\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=42\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=43\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=44\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=45\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=46\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=47\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=48\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=49\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=50\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=51\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([52, 53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=52\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([53, 54, 55, 56, 57, 58, 59])\n",
      "consider neuron=53\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([54, 55, 56, 57, 58, 59])\n",
      "consider neuron=54\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "sat\n",
      "input 0 = 0.1951652894771927\n",
      "input 1 = 0.631354532538259\n",
      "input 2 = 0.828367182642942\n",
      "input 3 = 0.6518824097528227\n",
      "output 0 = -3.8936185774322234\n",
      "output 1 = 2.0330503390599812\n",
      "output 2 = 2.0320503390599787\n",
      "consider neuron=55\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([54, 56, 57, 58, 59])\n",
      "consider neuron=56\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([54, 57, 58, 59])\n",
      "consider neuron=57\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([54, 58, 59])\n",
      "consider neuron=58\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([54, 59])\n",
      "consider neuron=59\n",
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n",
      "dict_keys([54])\n",
      "Remaining keys after coarsening for label 2: [54]\n"
     ]
    }
   ],
   "source": [
    "def func(relus):\n",
    "    for other_label in range(3):\n",
    "        exit_code, running_time = check_pattern(relus, label=int(label), other_label=other_label)\n",
    "        if exit_code == \"sat\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def scancoarsen(relu_label):\n",
    "    keys = list(relu_label.keys())  # Convert keys to a list to avoid runtime error while modifying the dictionary\n",
    "    for k in keys:\n",
    "        print(f\"consider neuron={k}\")\n",
    "        vals = relu_label[k]\n",
    "        relu_label_copy = relu_label.copy()\n",
    "        relu_label_copy.pop(k)\n",
    "        if func(relu_label_copy):\n",
    "            relu_label.pop(k)\n",
    "            print(relu_label.keys())\n",
    "            continue\n",
    "        else:\n",
    "            relu_label[k] = vals\n",
    "    return list(relu_label.keys())  # Return the remaining keys\n",
    "\n",
    "# # Assuming STABLE_PATTERNS is a list of dictionaries\n",
    "# for label in range(1):\n",
    "#     relu_label = STABLE_PATTERNS[label].copy()\n",
    "#     remaining_keys = scancoarsen(relu_label)\n",
    "#     print(f\"Remaining keys after coarsening for label {label}: {remaining_keys}\")\n",
    "\n",
    "    \n",
    "label = 2\n",
    "relu_label = STABLE_PATTERNS[label].copy()\n",
    "remaining_keys = scancoarsen(relu_label)\n",
    "print(f\"Remaining keys after coarsening for label {label}: {remaining_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------CHECK PATTERN: output_2 is always less than output_0 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_1 ? --------\n",
      "unsat\n",
      "--------CHECK PATTERN: output_2 is always less than output_2 ? --------\n",
      "unsat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_label = STABLE_PATTERNS[label].copy()\n",
    "res_dict = dict((k, relu_label[k]) for k in [54])\n",
    "type(res_dict)\n",
    "res_dict\n",
    "\n",
    "func(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relu_label = STABLE_PATTERNS[label].copy()\n",
    "type(relu_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relu_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0]\n",
      " [1 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 1 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 1]]\n",
      "[0 1 0 1 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [1, 1, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_samples(n, k, alpha, j):\n",
    "    # Calculate the number of samples with the jth element equal to 1\n",
    "    num_ones = int(alpha * n)\n",
    "    \n",
    "    # Generate n samples with the specified probability alpha\n",
    "    samples = np.random.choice([0, 1], size=(n, k), p=[0.5, 0.5])\n",
    "    print(samples)\n",
    "    j_col = np.random.choice([0, 1], size=(n, 1), p=[1-alpha, alpha]).reshape(n,)\n",
    "    print(j_col)\n",
    "    samples[:,j] = j_col\n",
    "    \n",
    "    # Set the jth element to 1 in the required number of samples\n",
    "    \n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Example usage:\n",
    "n = 10  # Number of samples\n",
    "k = 4   # Length of the list\n",
    "alpha = 0.1  # Probability\n",
    "j = 1   # Index of the element to be set to 1\n",
    "\n",
    "generated_samples = generate_samples(n, k, alpha, j)\n",
    "generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1],\n",
       "       [1, 0, 0, 1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_samples[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STABLE_PATTERNS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m relu_label\u001b[38;5;241m=\u001b[39m\u001b[43mSTABLE_PATTERNS\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m relu_label\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STABLE_PATTERNS' is not defined"
     ]
    }
   ],
   "source": [
    "relu_label=STABLE_PATTERNS[1].copy()\n",
    "relu_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgG40QhIS2rs"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu5sLx1rFlC5",
    "outputId": "75266370-ecab-48ae-ab4e-8a13d3e2d649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin1.weight tensor([[ 0.0055,  0.4843],\n",
      "        [ 0.7000, -0.0224],\n",
      "        [ 0.2257, -0.3779],\n",
      "        [-0.3030, -0.6275]])\n",
      "lin1.bias tensor([ 0.6780,  0.5154, -0.2446, -0.3855])\n",
      "lin2.weight tensor([[ 0.2899,  0.1738, -0.3707, -0.0071],\n",
      "        [ 0.1005,  0.4264,  0.0765, -0.0099]])\n",
      "lin2.bias tensor([-0.2550, -0.1218])\n",
      "lin3.weight tensor([[-0.3563, -0.6455]])\n",
      "lin3.bias tensor([0.4740])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('lin1', nn.Linear(2, 4)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('lin2', nn.Linear(4, 2)),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('lin3', nn.Linear(2, 1)),\n",
    "           ('sigmoid1',nn.Sigmoid())\n",
    "          # ('relu3', nn.ReLU()) # this will make the gradient always zero for some initializations\n",
    "        ]))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "E-7x-NvoFqCG",
    "outputId": "9137bfda-ab9e-47fe-da71-14de1dafc436"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5zklEQVR4nO3df3hU1YH/8c9kgIQgk4AJ+UGCAXERKghCE0PNQkseEmS3sIEKSMuPpdBSURBUSL+CAraguC5Q2WJdfj4VUXmCWtem0GjatMZgQapi4AEXTAKZIFAyBDSEyfn+kc3ImBCSkMlMbt6v57lPnHPPvXNOLjAf75xzrs0YYwQAAGAhQf5uAAAAQEsj4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMvp4O8G+EN1dbVOnTqlrl27ymaz+bs5AACgEYwxunDhgmJjYxUU1PA9mnYZcE6dOqX4+Hh/NwMAADRDcXGx4uLiGqzTLgNO165dJdX8ghwOh59bAwAAGsPlcik+Pt7zOd6Qdhlwar+WcjgcBBwAANqYxgwvYZAxAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwnHa50B+AGu5qt/KK8lR6oVQxXWOU0itF9iC7v5sFADeMgAO0U1mFWZqfPV8lrhJPWZwjTuvS1ymjf4YfWwYAN46vqIB2KKswSxNfnegVbiTppOukJr46UVmFWX5qGQC0DAIO0M64q92anz1fRqbOvtqyBdkL5K52t3bTAKDFEHCAdiavKK/OnZurGRkVu4qVV5TXiq0CgJZFwAHamdILpS1aDwACEQEHaGdiusa0aD0ACEQEHKCdSemVojhHnGyy1bvfJpviHfFK6ZXSyi0DgJZDwAHaGXuQXevS10lSnZBT+3pt+lrWwwHQphFwgHYoo3+Gdt23Sz0dPb3K4xxx2nXfLtbBAdDm2YwxdeeKWpzL5VJYWJjKy8vlcDj83RzAb1jJGEBb0pTP71a5g7NhwwYlJCQoJCRESUlJ2rdv3zXrjhw5Ujabrc42duxYT50ZM2bU2Z+ent4aXQEsxR5k18iEkZoycIpGJowk3ACwDJ8/quGVV17RwoULtXHjRiUlJWnt2rVKS0vTkSNH1KNHjzr1s7KydPnyZc/rs2fP6s4779QPfvADr3rp6enasmWL53VwcLDvOgEAANoUn9/Bee655zR79mzNnDlTAwYM0MaNGxUaGqrNmzfXW7979+6Kjo72bHv37lVoaGidgBMcHOxVr1u3br7uCgAAaCN8GnAuX76s/fv3KzU19es3DApSamqq8vPzG3WOTZs2afLkyerSpYtXeW5urnr06KF+/fpp7ty5Onv27DXPUVlZKZfL5bUBAADr8mnAOXPmjNxut6KiorzKo6Ki5HQ6r3v8vn379Mknn+jHP/6xV3l6erq2b9+unJwcPf300/rTn/6kMWPGyO2u/9k5q1atUlhYmGeLj49vfqcAAEDA8/kYnBuxadMmDRw4UImJiV7lkydP9vz3wIEDNWjQIN16663Kzc3VqFGj6pwnMzNTCxcu9Lx2uVyEHAAALMynd3AiIiJkt9tVVlbmVV5WVqbo6OgGj7148aJ27typWbNmXfd9+vTpo4iICB07dqze/cHBwXI4HF4bAACwLp8GnE6dOmno0KHKycnxlFVXVysnJ0fJyckNHvvaa6+psrJSP/zhD6/7PiUlJTp79qxiYnh2DgAAaIVZVAsXLtSLL76obdu2qbCwUHPnztXFixc1c+ZMSdK0adOUmZlZ57hNmzZp/Pjxuvnmm73KKyoq9Oijj+r999/XiRMnlJOTo3Hjxqlv375KS0vzdXcAAEAb4PMxOJMmTdIXX3yhZcuWyel0avDgwcrOzvYMPC4qKlJQkHfOOnLkiP7yl79oz549dc5nt9v10Ucfadu2bTp//rxiY2M1evRorVy5krVwAACAJB7VwHgcAADaiIB7VAMAAEBrIuAAAADLCeh1cAC0HJ4cDqA9IeAA7UBWYZbmZ89XiavEUxbniNO69HXK6J/hx5YBgG/wFRVgcVmFWZr46kSvcCNJJ10nNfHVicoqzPJTywDAdwg4gIW5q92anz1fRnUnS9aWLcheIHd1/c9xA4C2ioADWFheUV6dOzdXMzIqdhUrryivFVsFAL5HwAEsrPRCaYvWA4C2goADWFhM18Y9n62x9QCgrSDgABaW0itFcY442WSrd79NNsU74pXSK6WVWwYAvkXAASzMHmTXuvR1klQn5NS+Xpu+lvVwAFgOAQewuIz+Gdp13y71dPT0Ko9zxGnXfbtYBweAJfGwTR62iXaClYwBtHVN+fxmJWOgnbAH2TUyYaS/mwEArYKvqAAAgOUQcAAAgOUQcAAAgOUQcAAAgOUQcAAAgOUQcAAAgOUwTdxCWOcEAIAaBByLyCrM0vzs+SpxlXjK4hxxWpe+jpVqAQDtDl9RWUBWYZYmvjrRK9xI0knXSU18daKyCrP81DIAAPyDgNPGuavdmp89X0Z1n7hRW7Yge4Hc1e7WbhoAAH5DwGnj8ory6ty5uZqRUbGrWHlFea3YKgAA/IuA08aVXiht0XoAAFgBAaeNi+ka06L1AACwAgJOG5fSK0VxjjjZZKt3v002xTvildIrpZVbBgCA/xBw2jh7kF3r0tdJUp2QU/t6bfpa1sMBALQrBBwLyOifoV337VJPR0+v8jhHnHbdt4t1cAAA7Y7NGFN3frHFuVwuhYWFqby8XA6Hw9/NaTGsZAwAsLKmfH6zkrGF2IPsGpkw0t/NAADA71rlK6oNGzYoISFBISEhSkpK0r59+65Zd+vWrbLZbF5bSEiIVx1jjJYtW6aYmBh17txZqampOnr0qK+7AQAA2gifB5xXXnlFCxcu1BNPPKEDBw7ozjvvVFpamk6fPn3NYxwOh0pLSz3b559/7rX/mWee0fr167Vx40YVFBSoS5cuSktL01dffeXr7gAAgDbA5wHnueee0+zZszVz5kwNGDBAGzduVGhoqDZv3nzNY2w2m6Kjoz1bVFSUZ58xRmvXrtXjjz+ucePGadCgQdq+fbtOnTql119/3dfdAQAAbYBPA87ly5e1f/9+paamfv2GQUFKTU1Vfn7+NY+rqKjQLbfcovj4eI0bN06HDh3y7Dt+/LicTqfXOcPCwpSUlHTNc1ZWVsrlcnltAADAunwacM6cOSO32+11B0aSoqKi5HQ66z2mX79+2rx5s9544w399re/VXV1tYYPH66SkprnLdUe15Rzrlq1SmFhYZ4tPj7+RrsGAAACWMCtg5OcnKxp06Zp8ODBGjFihLKyshQZGakXXnih2efMzMxUeXm5ZysuLm7BFgMAgEDj04ATEREhu92usrIyr/KysjJFR0c36hwdO3bUkCFDdOzYMUnyHNeUcwYHB8vhcHhtAADAunwacDp16qShQ4cqJyfHU1ZdXa2cnBwlJyc36hxut1sff/yxYmJqHhbZu3dvRUdHe53T5XKpoKCg0ecEAADW5vOF/hYuXKjp06dr2LBhSkxM1Nq1a3Xx4kXNnDlTkjRt2jT17NlTq1atkiStWLFCd999t/r27avz589rzZo1+vzzz/XjH/9YUs0MqwULFuipp57Sbbfdpt69e2vp0qWKjY3V+PHjfd2dFsOqwwAA+I7PA86kSZP0xRdfaNmyZXI6nRo8eLCys7M9g4SLiooUFPT1jaR//OMfmj17tpxOp7p166ahQ4fqvffe04ABAzx1HnvsMV28eFFz5szR+fPndc899yg7O7vOgoCBKqswS/Oz56vEVeIpi3PEaV36Op4bBQBAC+BZVK08HierMEsTX50oI+9fe+2Tv3k4JgAA9WvK53fAzaKyMne1W/Oz59cJN5I8ZQuyF8hd7W7tpgEAYCkEnFaUV5Tn9bXUNxkZFbuKlVeU14qtAgDAegg4raj0QmmL1gMAAPUj4LSimK4xLVoPAADUj4DTilJ6pSjOEecZUPxNNtkU74hXSq+UVm4ZAADWQsBpRfYgu9alr5OkOiGn9vXa9LWshwMAwA0i4LSyjP4Z2nXfLvV09PQqj3PEBdwUcXe1W7kncvXyxy8r90Qus7sAAG0G6+D46blUgb6SMYsRAgACTVM+vwk4PHizDhYjBAAEIhb6Q7OxGCEAwAoIOPDCYoQAACvw+cM20XJaY9wOixECAKyAgNNGtNagXxYjbLxAHygOAO0ZAacNuNag35Ouk5r46sQWHfRbuxjhSdfJesfh2GRTnCOu3S9GyCwzAAhsjMEJcK096JfFCK+vNnB+c6xSbeDMKszyU8sAALUIOAHOH4N+29JihK2NWWYA0DbwFVWA89eg34z+GRrXbxxjTL6hKYFzZMLI1msYAMALASfA+XPQrz3Izof0NzDLDADaBr6iCnA8gTywMMsMANoGAk6AY9BvYCFwAkDbQMBpAxj0GzgInADQNvCwzTb0sE0Wlgsc9a2DE++I19r0tQROAPARniZ+HW014CCwEDgBoHU15fObWVRAMzHLDAACF2NwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5bRKwNmwYYMSEhIUEhKipKQk7du375p1X3zxRaWkpKhbt27q1q2bUlNT69SfMWOGbDab15aenu7rbgAAgDbC5wHnlVde0cKFC/XEE0/owIEDuvPOO5WWlqbTp0/XWz83N1dTpkzRu+++q/z8fMXHx2v06NE6efKkV7309HSVlpZ6tpdfftnXXQEAAG2Ezx/VkJSUpG9/+9t6/vnnJUnV1dWKj4/Xgw8+qCVLllz3eLfbrW7duun555/XtGnTJNXcwTl//rxef/31ZrWJRzUAAND2NOXz26d3cC5fvqz9+/crNTX16zcMClJqaqry8/MbdY5Lly6pqqpK3bt39yrPzc1Vjx491K9fP82dO1dnz5695jkqKyvlcrm8NgAAYF0+DThnzpyR2+1WVFSUV3lUVJScTmejzrF48WLFxsZ6haT09HRt375dOTk5evrpp/WnP/1JY8aMkdvtrvccq1atUlhYmGeLj49vfqcAAEDAC+iHba5evVo7d+5Ubm6uQkJCPOWTJ0/2/PfAgQM1aNAg3XrrrcrNzdWoUaPqnCczM1MLFy70vHa5XIQcAAAszKd3cCIiImS321VWVuZVXlZWpujo6AaPffbZZ7V69Wrt2bNHgwYNarBunz59FBERoWPHjtW7Pzg4WA6Hw2sDAADW5dOA06lTJw0dOlQ5OTmesurqauXk5Cg5Ofmaxz3zzDNauXKlsrOzNWzYsOu+T0lJic6ePauYmJgWaTcAAGjbfD5NfOHChXrxxRe1bds2FRYWau7cubp48aJmzpwpSZo2bZoyMzM99Z9++mktXbpUmzdvVkJCgpxOp5xOpyoqKiRJFRUVevTRR/X+++/rxIkTysnJ0bhx49S3b1+lpaX5ujsAAKAN8PkYnEmTJumLL77QsmXL5HQ6NXjwYGVnZ3sGHhcVFSko6Ouc9etf/1qXL1/WxIkTvc7zxBNP6Mknn5TdbtdHH32kbdu26fz584qNjdXo0aO1cuVKBQcH+7o7AACgDfD5OjiBiHVwAABoewJmHRwAAAB/IOAAAADLIeAAAADLIeAAAADLIeAAAADLIeAAAADLIeAAAADLIeAAAADLIeAAAADLIeAAAADL8fmzqNoTd7VbeUV5Kr1QqpiuMUrplSJ7kN3fzQIAoN0h4LSQrMIszc+erxJXiacszhGndenrlNE/w48tAwCg/eErqhaQVZilia9O9Ao3knTSdVITX52orMIsP7UMAID2iYBzg9zVbs3Pni+jug9lry1bkL1A7mp3azcNAIB2i4Bzg/KK8urcubmakVGxq1h5RXmt2CoAANo3As4NKr1Q2qL1AADAjSPg3KCYrjEtWg8AANw4As4NSumVojhHnGyy1bvfJpviHfFK6ZXSyi0DAKD9IuDcIHuQXevS10lSnZBT+3pt+lrWwwEAoBURcFpARv8M7bpvl3o6enqVxznitOu+XayDAwBAK7MZY+rOb7Y4l8ulsLAwlZeXy+FwtNh5WckYAADfacrnNysZtyB7kF0jE0b6uxmWR5AEAFwPAQdtCo/EAAA0BmNw0GbwSAwAQGMRcNAm8EgMAGgj3G4pN1d6+eWan27//LtMwEGbwCMxAKANyMqSEhKk735Xuv/+mp8JCTXlrYyA0064q93KPZGrlz9+WbknctvcnQ4eiQEAAS4rS5o4USr5xv+MnjxZU97KIYdBxu2AFQbm8kgMAAhgbrc0f75U38ozxkg2m7RggTRunGRvnVmv3MGxOKsMzOWRGAAQwPLy6t65uZoxUnFxTb1WQsCxMCsNzOWRGAAQwEobOTygsfVaAAHHwqw2MJdHYgBAgIpp5PCAxtZrAYzBsTArDszN6J+hcf3GsZIxAASSlBQpLq5mQHF943Bstpr9Ka03jKBV7uBs2LBBCQkJCgkJUVJSkvbt29dg/ddee0233367QkJCNHDgQL399tte+40xWrZsmWJiYtS5c2elpqbq6NGjvuxCm2TVgbm1j8SYMnCKRiaMJNwAgL/Z7dK6mmEEsn1jrGTt67VrW22AsdQKAeeVV17RwoUL9cQTT+jAgQO68847lZaWptOnT9db/7333tOUKVM0a9Ysffjhhxo/frzGjx+vTz75xFPnmWee0fr167Vx40YVFBSoS5cuSktL01dffeXr7rQpDMwFALSajAxp1y6pp/cwAsXF1ZRntO4wAp8/TTwpKUnf/va39fzzz0uSqqurFR8frwcffFBLliypU3/SpEm6ePGi3nrrLU/Z3XffrcGDB2vjxo0yxig2NlaLFi3SI488IkkqLy9XVFSUtm7dqsmTJ1+3Tb56mnggqp1FJclrsHFt6GHsCgCgRbndNbOlSktrxtykpLTYnZumfH779A7O5cuXtX//fqWmpn79hkFBSk1NVX5+fr3H5Ofne9WXpLS0NE/948ePy+l0etUJCwtTUlLSNc9ZWVkpl8vltbUXDMwFALQqu10aOVKaMqXmZyt+LXU1nw4yPnPmjNxut6KiorzKo6KidPjw4XqPcTqd9dZ3Op2e/bVl16rzTatWrdLy5cub1QcrYGAuAKC9aRezqDIzM7Vw4ULPa5fLpfj4eD+2qPXVDswFAKA98OlXVBEREbLb7SorK/MqLysrU3R0dL3HREdHN1i/9mdTzhkcHCyHw+G1AQAA6/JpwOnUqZOGDh2qnJwcT1l1dbVycnKUnJxc7zHJycle9SVp7969nvq9e/dWdHS0Vx2Xy6WCgoJrnhMAALQvPv+KauHChZo+fbqGDRumxMRErV27VhcvXtTMmTMlSdOmTVPPnj21atUqSdL8+fM1YsQI/cd//IfGjh2rnTt36m9/+5t+85vfSJJsNpsWLFigp556Srfddpt69+6tpUuXKjY2VuPHj/d1dwAAQBvg84AzadIkffHFF1q2bJmcTqcGDx6s7OxszyDhoqIiBQV9fSNp+PDh2rFjhx5//HH9/Oc/12233abXX39dd9xxh6fOY489posXL2rOnDk6f/687rnnHmVnZyskJMTX3QEAAG2Az9fBCUTtaR0cAACsImDWwQEAAPAHAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcnwacc+fOaerUqXI4HAoPD9esWbNUUVHRYP0HH3xQ/fr1U+fOndWrVy899NBDKi8v96pns9nqbDt37vRlVwAAQBvSwZcnnzp1qkpLS7V3715VVVVp5syZmjNnjnbs2FFv/VOnTunUqVN69tlnNWDAAH3++ef66U9/qlOnTmnXrl1edbds2aL09HTP6/DwcF92BQAAtCE2Y4zxxYkLCws1YMAAffDBBxo2bJgkKTs7W/fee69KSkoUGxvbqPO89tpr+uEPf6iLFy+qQ4eaPGaz2bR7926NHz++WW1zuVwKCwtTeXm5HA5Hs84BAABaV1M+v332FVV+fr7Cw8M94UaSUlNTFRQUpIKCgkafp7YTteGm1gMPPKCIiAglJiZq8+bNaiinVVZWyuVyeW0AAMC6fPYVldPpVI8ePbzfrEMHde/eXU6ns1HnOHPmjFauXKk5c+Z4la9YsULf+973FBoaqj179uhnP/uZKioq9NBDD9V7nlWrVmn58uXN6wgAAGhzmnwHZ8mSJfUO8r16O3z48A03zOVyaezYsRowYICefPJJr31Lly7Vd77zHQ0ZMkSLFy/WY489pjVr1lzzXJmZmSovL/dsxcXFN9w+AAAQuJp8B2fRokWaMWNGg3X69Omj6OhonT592qv8ypUrOnfunKKjoxs8/sKFC0pPT1fXrl21e/dudezYscH6SUlJWrlypSorKxUcHFxnf3BwcL3lAADAmpoccCIjIxUZGXndesnJyTp//rz279+voUOHSpLeeecdVVdXKykp6ZrHuVwupaWlKTg4WG+++aZCQkKu+14HDx5Ut27dCDEAAECSD8fg9O/fX+np6Zo9e7Y2btyoqqoqzZs3T5MnT/bMoDp58qRGjRql7du3KzExUS6XS6NHj9alS5f029/+1mtAcGRkpOx2u373u9+prKxMd999t0JCQrR371798pe/1COPPOKrrgAAgDbGp+vgvPTSS5o3b55GjRqloKAgTZgwQevXr/fsr6qq0pEjR3Tp0iVJ0oEDBzwzrPr27et1ruPHjyshIUEdO3bUhg0b9PDDD8sYo759++q5557T7NmzfdkVAADQhvhsHZxAxjo4AAC0PQGxDg4AAIC/EHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHAAAIDldPB3AwAAsBS3W8rLk0pLpZgYKSVFstv93ap2h4ADAEBLycqS5s+XSkq+LouLk9atkzIy/NeudoivqAAAaAlZWdLEid7hRpJOnqwpz8ryT7vaKQIOAAA3yu2uuXNjTN19tWULFtTUQ6sg4AAAcKPy8ureubmaMVJxcU09tArG4AAA2reWGBRcWtqy9XDDCDgAgParpQYFx8S0bD3cML6iAgC0Ty05KDglpSYY2Wz177fZpPj4mnpoFQQcAED709KDgu32mrs+Ut2QU/t67VrWw2lFBBwAQPvji0HBGRnSrl1Sz57e5XFxNeWsg9OqGIMDAGh/fDUoOCNDGjeOlYwDAAEHAND++HJQsN0ujRzZ9OPQoviKCgDQ/jAo2PIIOACA9odBwZZHwAEAtE8MCrY0xuAAANovBgVbFgEHANC+MSjYkgg4AAAEupZ4XlY7Q8ABACCQtdTzstoZnw4yPnfunKZOnSqHw6Hw8HDNmjVLFRUVDR4zcuRI2Ww2r+2nP/2pV52ioiKNHTtWoaGh6tGjhx599FFduXLFl10BAKD1teTzstoZn97BmTp1qkpLS7V3715VVVVp5syZmjNnjnbs2NHgcbNnz9aKFSs8r0NDQz3/7Xa7NXbsWEVHR+u9995TaWmppk2bpo4dO+qXv/ylz/oCAECrut7zsmy2mudljRvH11X18NkdnMLCQmVnZ+u///u/lZSUpHvuuUe/+tWvtHPnTp06darBY0NDQxUdHe3ZHA6HZ9+ePXv06aef6re//a0GDx6sMWPGaOXKldqwYYMuX77sq+4AANC6fPG8rHbEZwEnPz9f4eHhGjZsmKcsNTVVQUFBKigoaPDYl156SREREbrjjjuUmZmpS5cueZ134MCBioqK8pSlpaXJ5XLp0KFD9Z6vsrJSLpfLawMAIKD56nlZ7YTPvqJyOp3q0aOH95t16KDu3bvL6XRe87j7779ft9xyi2JjY/XRRx9p8eLFOnLkiLL+73tGp9PpFW4keV5f67yrVq3S8uXLb6Q7AAC0Ll8+L6sdaPIdnCVLltQZBPzN7fDhw81u0Jw5c5SWlqaBAwdq6tSp2r59u3bv3q3PPvus2efMzMxUeXm5ZysuLm72uQAAaBU8L+uGNPkOzqJFizRjxowG6/Tp00fR0dE6ffq0V/mVK1d07tw5RUdHN/r9kpKSJEnHjh3TrbfequjoaO3bt8+rTllZmSRd87zBwcEKDg5u9HsCAOB3tc/LmjixJsxcPdiY52VdV5MDTmRkpCIjI69bLzk5WefPn9f+/fs1dOhQSdI777yj6upqT2hpjIMHD0qSYv7vFlxycrJ+8Ytf6PTp056vwPbu3SuHw6EBAwY0sTcAAASw2udl1bcOztq1rIPTAJsx9c0/axljxoxRWVmZNm7c6JkmPmzYMM808ZMnT2rUqFHavn27EhMT9dlnn2nHjh269957dfPNN+ujjz7Sww8/rLi4OP3pT3+SVDNNfPDgwYqNjdUzzzwjp9OpH/3oR/rxj3/c6GniLpdLYWFhKi8v95qhBQBAQGIlY0lN+/z26To4L730kubNm6dRo0YpKChIEyZM0Pr16z37q6qqdOTIEc8sqU6dOumPf/yj1q5dq4sXLyo+Pl4TJkzQ448/7jnGbrfrrbfe0ty5c5WcnKwuXbpo+vTpXuvmAABgKTwvq8l8egcnUHEHBwCAtqcpn98+fVQDAACAPxBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5XTwdwMAAG2A2y3l5UmlpVJMjJSSItnt/m4VcE0EHABAw7KypPnzpZKSr8vi4qR166SMDN+9L6EKN4CvqAAA15aVJU2c6B1uJOnkyZryrCzfvW9CgvTd70r331/zMyHBd+8HyyHgAADq53bX3Lkxpu6+2rIFC2rqtSR/hSpYCgEHAFC/vLy6IeNqxkjFxTX1Woq/QhUsh4ADAKhfaWnL1msMf4QqWBIBBwBQv5iYlq3XGP4IVbAkAg4AoH4pKTWzpWy2+vfbbFJ8fE29luKPUAVLIuAAgBW53VJurvTyyzU/mzNmxW6vmQou1Q05ta/Xrm3Zqdv+CFWwJAIOAFhNS06xzsiQdu2Sevb0Lo+Lqylv6XVw/BGqYEk2Y+obqm5tLpdLYWFhKi8vl8Ph8HdzAKDl1E6x/uY/7bXhoLmhpLmL7jX3uPoWF4yPrwk3vlxcEAGtKZ/fBBwCDgCrcLtr7tRcaxaSzVZz5+X48da5A3KjKyCzkjG+oSmf3zyqAQCsoilTrEeO9G1brnUnqXaxvsbcSbLbfd9OWBZjcADAKgJlijWL9SEAEHAAwCoCZYo1i/UhABBwAMAqAmWKdaDcSUK7RsABAKsIlCnWgXInCe2aTwPOuXPnNHXqVDkcDoWHh2vWrFmqqKi4Zv0TJ07IZrPVu7322mueevXt37lzpy+7AgBtQ2uvW1OfQLmThHbNp9PEx4wZo9LSUr3wwguqqqrSzJkz9e1vf1s7duyot77b7dYXX3zhVfab3/xGa9asUWlpqW666aaaRtts2rJli9LT0z31wsPDFRIS0qh2MU0cgOX5e4p17SwqyXuw8Y2ux4N2LSDWwSksLNSAAQP0wQcfaNiwYZKk7Oxs3XvvvSopKVFsbGyjzjNkyBDddddd2rRp09eNttm0e/dujR8/vlltI+AAQCtgsT60sKZ8fvvsK6r8/HyFh4d7wo0kpaamKigoSAUFBY06x/79+3Xw4EHNmjWrzr4HHnhAERERSkxM1ObNm9VQTqusrJTL5fLaAAA+lpEhnTghvfuutGNHzc/jxwk3aBU+W+jP6XSqR48e3m/WoYO6d+8up9PZqHNs2rRJ/fv31/Dhw73KV6xYoe9973sKDQ3Vnj179LOf/UwVFRV66KGH6j3PqlWrtHz58uZ1BADQfCzWBz9p8h2cJUuWXHMgcO12+PDhG27Yl19+qR07dtR792bp0qX6zne+oyFDhmjx4sV67LHHtGbNmmueKzMzU+Xl5Z6tuLj4htsHAAACV5Pv4CxatEgzZsxosE6fPn0UHR2t06dPe5VfuXJF586dU3R09HXfZ9euXbp06ZKmTZt23bpJSUlauXKlKisrFRwcXGd/cHBwveUAAMCamhxwIiMjFRkZed16ycnJOn/+vPbv36+hQ4dKkt555x1VV1crKSnpusdv2rRJ3//+9xv1XgcPHlS3bt0IMQAAQJIPx+D0799f6enpmj17tjZu3KiqqirNmzdPkydP9sygOnnypEaNGqXt27crMTHRc+yxY8f05z//WW+//Xad8/7ud79TWVmZ7r77boWEhGjv3r365S9/qUceecRXXQEAAG2MT58m/tJLL2nevHkaNWqUgoKCNGHCBK1fv96zv6qqSkeOHNGlS5e8jtu8ebPi4uI0evToOufs2LGjNmzYoIcffljGGPXt21fPPfecZs+e7cuuAACANsSnC/0FKtbBAQCg7QmIdXAAAAD8hYADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsh4ADAAAsx2cB5xe/+IWGDx+u0NBQhYeHN+oYY4yWLVummJgYde7cWampqTp69KhXnXPnzmnq1KlyOBwKDw/XrFmzVFFR4YMeAACAtspnAefy5cv6wQ9+oLlz5zb6mGeeeUbr16/Xxo0bVVBQoC5duigtLU1fffWVp87UqVN16NAh7d27V2+99Zb+/Oc/a86cOb7oAgAAaKNsxhjjyzfYunWrFixYoPPnzzdYzxij2NhYLVq0SI888ogkqby8XFFRUdq6dasmT56swsJCDRgwQB988IGGDRsmScrOzta9996rkpISxcbGNqpNLpdLYWFhKi8vl8PhuKH+AQCA1tGUz++AGYNz/PhxOZ1OpaamesrCwsKUlJSk/Px8SVJ+fr7Cw8M94UaSUlNTFRQUpIKCgmueu7KyUi6Xy2sDAADWFTABx+l0SpKioqK8yqOiojz7nE6nevTo4bW/Q4cO6t69u6dOfVatWqWwsDDPFh8f38KtBwAAgaRJAWfJkiWy2WwNbocPH/ZVW5stMzNT5eXlnq24uNjfTQIAAD7UoSmVFy1apBkzZjRYp0+fPs1qSHR0tCSprKxMMTExnvKysjINHjzYU+f06dNex125ckXnzp3zHF+f4OBgBQcHN6tdAACg7WlSwImMjFRkZKRPGtK7d29FR0crJyfHE2hcLpcKCgo8M7GSk5N1/vx57d+/X0OHDpUkvfPOO6qurlZSUpJP2gUAANoen43BKSoq0sGDB1VUVCS3262DBw/q4MGDXmvW3H777dq9e7ckyWazacGCBXrqqaf05ptv6uOPP9a0adMUGxur8ePHS5L69++v9PR0zZ49W/v27dNf//pXzZs3T5MnT270DCoAAGB9TbqD0xTLli3Ttm3bPK+HDBkiSXr33Xc1cuRISdKRI0dUXl7uqfPYY4/p4sWLmjNnjs6fP6977rlH2dnZCgkJ8dR56aWXNG/ePI0aNUpBQUGaMGGC1q9f76tuAMD1ud1SXp5UWirFxEgpKZLd7u9WAe2az9fBCUSsgwOgxWRlSfPnSyUlX5fFxUnr1kkZGf5rF2BBbXIdHABoc7KypIkTvcONJJ08WVOeleWfdgEg4ABAs7jdNXdu6rsJXlu2YEFNPQCtjoADAM2Rl1f3zs3VjJGKi2vqAWh1BBwAaI7S0patB6BFEXAAoDmuWpC0ReoBaFEEHABojpSUmtlSNlv9+202KT6+ph6AVkfAAYDmsNtrpoJLdUNO7eu1a1kPB/ATAg4ANFdGhrRrl9Szp3d5XFxNOevgAH7js5WMAaBdyMiQxo1jJWMgwBBwAOBG2e3S/z2CBkBg4CsqAABgOQQcAABgOQQcAABgOQQcAABgOQQcAABgOQQcAABgOUwTB4CmcLtZ8wZoAwg4ANBYWVnS/PlSScnXZXFxNY9sYNViIKDwFRUANEZWljRxone4kaSTJ2vKs7L80y4A9SLgAMD1uN01d26MqbuvtmzBgpp6AAICAQcAricvr+6dm6sZIxUX19QDEBAIOABwPaWlLVsPgM8RcADgemJiWrYeAJ8j4ADA9aSk1MyWstnq32+zSfHxNfUABAQCDgBcj91eMxVcqhtyal+vXct6OEAAIeAAQGNkZEi7dkk9e3qXx8XVlLMODhBQWOgPABorI0MaN46VjIE2gIADAE1ht0sjR/q7FQCug6+oAACA5RBwAACA5RBwAACA5fgs4PziF7/Q8OHDFRoaqvDw8OvWr6qq0uLFizVw4EB16dJFsbGxmjZtmk6dOuVVLyEhQTabzWtbvXq1j3oBAADaIp8FnMuXL+sHP/iB5s6d26j6ly5d0oEDB7R06VIdOHBAWVlZOnLkiL7//e/XqbtixQqVlpZ6tgcffLClmw8AANown82iWr58uSRp69atjaofFhamvXv3epU9//zzSkxMVFFRkXr16uUp79q1q6Kjo1usrQAAwFoCegxOeXm5bDZbna+4Vq9erZtvvllDhgzRmjVrdOXKlQbPU1lZKZfL5bUBAADrCth1cL766istXrxYU6ZMkcPh8JQ/9NBDuuuuu9S9e3e99957yszMVGlpqZ577rlrnmvVqlWeO0oAAMD6bMYY09jKS5Ys0dNPP91gncLCQt1+++2e11u3btWCBQt0/vz5RjeqqqpKEyZMUElJiXJzc70Czjdt3rxZP/nJT1RRUaHg4OB661RWVqqystLz2uVyKT4+XuXl5Q2eGwAABA6Xy6WwsLBGfX436Q7OokWLNGPGjAbr9OnTpymnrKOqqkr33XefPv/8c73zzjvX7UBSUpKuXLmiEydOqF+/fvXWCQ4O9go/tZmOr6oAAGg7aj+3G3NvpkkBJzIyUpGRkc1rVSPUhpujR4/q3Xff1c0333zdYw4ePKigoCD16NGj0e9z4cIFSVJ8fHyz2woAAPzjwoULCgsLa7COz8bgFBUV6dy5cyoqKpLb7dbBgwclSX379tVNN90kSbr99tu1atUq/du//Zuqqqo0ceJEHThwQG+99ZbcbrecTqckqXv37urUqZPy8/NVUFCg7373u+ratavy8/P18MMP64c//KG6devW6LbFxsaquLhYXbt2lc1ma/G+36jar9CKi4vbxVdo9Nfa2lt/pfbXZ/prbYHUX2OMLly4oNjY2OvW9VnAWbZsmbZt2+Z5PWTIEEnSu+++q5H/96C6I0eOqLy8XJJ08uRJvfnmm5KkwYMHe52r9pjg4GDt3LlTTz75pCorK9W7d289/PDDWrhwYZPaFhQUpLi4uGb2rPU4HA6//2FqTfTX2tpbf6X212f6a22B0t/r3bmp5bOAs3Xr1uuugXP1d2gJCQnX/U7trrvu0vvvv98SzQMAABYW0OvgAAAANAcBJwAFBwfriSeeuOa0d6uhv9bW3vortb8+019ra6v9bdI6OAAAAG0Bd3AAAIDlEHAAAIDlEHAAAIDlEHAAAIDlEHD85Ny5c5o6daocDofCw8M1a9YsVVRUXLP+iRMnZLPZ6t1ee+01T7369u/cubM1utSgpvZXkkaOHFmnLz/96U+96hQVFWns2LEKDQ1Vjx499Oijj+rKlSu+7EqjNLW/586d04MPPqh+/fqpc+fO6tWrlx566CHPQpi1AuX6btiwQQkJCQoJCVFSUpL27dvXYP3XXntNt99+u0JCQjRw4EC9/fbbXvuNMVq2bJliYmLUuXNnpaam6ujRo77sQpM0pb8vvviiUlJS1K1bN3Xr1k2pqal16s+YMaPOdUxPT/d1NxqtKf3dunVrnb6EhIR41bHS9a3v3yWbzaaxY8d66gTy9f3zn/+sf/3Xf1VsbKxsNptef/316x6Tm5uru+66S8HBwerbt2+9a9w19d+EVmHgF+np6ebOO+8077//vsnLyzN9+/Y1U6ZMuWb9K1eumNLSUq9t+fLl5qabbjIXLlzw1JNktmzZ4lXvyy+/bI0uNaip/TXGmBEjRpjZs2d79aW8vNyz/8qVK+aOO+4wqamp5sMPPzRvv/22iYiIMJmZmb7uznU1tb8ff/yxycjIMG+++aY5duyYycnJMbfddpuZMGGCV71AuL47d+40nTp1Mps3bzaHDh0ys2fPNuHh4aasrKze+n/961+N3W43zzzzjPn000/N448/bjp27Gg+/vhjT53Vq1ebsLAw8/rrr5u///3v5vvf/77p3bt3QPzZbWp/77//frNhwwbz4YcfmsLCQjNjxgwTFhZmSkpKPHWmT59u0tPTva7juXPnWqtLDWpqf7ds2WIcDodXX5xOp1cdK13fs2fPevX1k08+MXa73WzZssVTJ5Cv79tvv23+3//7fyYrK8tIMrt3726w/v/+7/+a0NBQs3DhQvPpp5+aX/3qV8Zut5vs7GxPnab+DlsLAccPPv30UyPJfPDBB56y3//+98Zms5mTJ082+jyDBw82//7v/+5V1pg/sK2tuf0dMWKEmT9//jX3v/322yYoKMjrH9Nf//rXxuFwmMrKyhZpe3O01PV99dVXTadOnUxVVZWnLBCub2JionnggQc8r91ut4mNjTWrVq2qt/59991nxo4d61WWlJRkfvKTnxhjjKmurjbR0dFmzZo1nv3nz583wcHB5uWXX/ZBD5qmqf39pitXrpiuXbuabdu2ecqmT59uxo0b19JNbRFN7e+WLVtMWFjYNc9n9ev7n//5n6Zr166moqLCUxbI1/dqjfn35LHHHjPf+ta3vMomTZpk0tLSPK9v9HfoK3xF5Qf5+fkKDw/XsGHDPGWpqakKCgpSQUFBo86xf/9+HTx4ULNmzaqz74EHHlBERIQSExO1efPmRj1W3pdupL8vvfSSIiIidMcddygzM1OXLl3yOu/AgQMVFRXlKUtLS5PL5dKhQ4daviON1BLXV5LKy8vlcDjUoYP3E1X8eX0vX76s/fv3KzU11VMWFBSk1NRU5efn13tMfn6+V32p5jrV1j9+/LicTqdXnbCwMCUlJV3znK2lOf39pkuXLqmqqkrdu3f3Ks/NzVWPHj3Ur18/zZ07V2fPnm3RtjdHc/tbUVGhW265RfHx8Ro3bpzX3z+rX99NmzZp8uTJ6tKli1d5IF7f5rje39+W+B36is+eRYVrczqd6tGjh1dZhw4d1L17d88T1K9n06ZN6t+/v4YPH+5VvmLFCn3ve99TaGio9uzZo5/97GeqqKjQQw891GLtb6rm9vf+++/XLbfcotjYWH300UdavHixjhw5oqysLM95rw43kjyvG/t79IWWuL5nzpzRypUrNWfOHK9yf1/fM2fOyO121/t7P3z4cL3HXOs61f4uan82VMdfmtPfb1q8eLFiY2O9PgDS09OVkZGh3r1767PPPtPPf/5zjRkzRvn5+bLb7S3ah6ZoTn/79eunzZs3a9CgQSovL9ezzz6r4cOH69ChQ4qLi7P09d23b58++eQTbdq0yas8UK9vc1zr76/L5dKXX36pf/zjHzf8d8RXCDgtaMmSJXr66acbrFNYWHjD7/Pll19qx44dWrp0aZ19V5cNGTJEFy9e1Jo1a3zyAejr/l794T5w4EDFxMRo1KhR+uyzz3Trrbc2+7zN1VrX1+VyaezYsRowYICefPJJr32teX1x41avXq2dO3cqNzfXa+Dt5MmTPf89cOBADRo0SLfeeqtyc3M1atQofzS12ZKTk5WcnOx5PXz4cPXv318vvPCCVq5c6ceW+d6mTZs0cOBAJSYmepVb6fq2ZQScFrRo0SLNmDGjwTp9+vRRdHS0Tp8+7VV+5coVnTt3TtHR0dd9n127dunSpUuaNm3adesmJSVp5cqVqqysbPHniLRWf2slJSVJko4dO6Zbb71V0dHRdUbql5WVSVKTzttYrdHfCxcuKD09XV27dtXu3bvVsWPHBuv78vrWJyIiQna73fN7rlVWVnbNvkVHRzdYv/ZnWVmZYmJivOoMHjy4BVvfdM3pb61nn31Wq1ev1h//+EcNGjSowbp9+vRRRESEjh075tcPwBvpb62OHTtqyJAhOnbsmCTrXt+LFy9q586dWrFixXXfJ1Cub3Nc6++vw+FQ586dZbfbb/jPjM/4dQRQO1U7CPVvf/ubp+wPf/hDowehjhgxos7smmt56qmnTLdu3Zrd1pZwo/2t9Ze//MVIMn//+9+NMV8PMr56pP4LL7xgHA6H+eqrr1quA03U3P6Wl5ebu+++24wYMcJcvHixUe/lj+ubmJho5s2b53ntdrtNz549Gxxk/C//8i9eZcnJyXUGGT/77LOe/eXl5QE1CLUp/TXGmKeffto4HA6Tn5/fqPcoLi42NpvNvPHGGzfc3hvVnP5e7cqVK6Zfv37m4YcfNsZY8/oaUzO4Ojg42Jw5c+a67xFI1/dqauQg4zvuuMOrbMqUKXUGGd/InxlfIeD4SXp6uhkyZIgpKCgwf/nLX8xtt93mNY24pKTE9OvXzxQUFHgdd/ToUWOz2czvf//7Oud88803zYsvvmg+/vhjc/ToUfNf//VfJjQ01Cxbtszn/bmepvb32LFjZsWKFeZvf/ubOX78uHnjjTdMnz59zD//8z97jqmdJj569Ghz8OBBk52dbSIjIwNmmnhT+lteXm6SkpLMwIEDzbFjx7yml165csUYEzjXd+fOnSY4ONhs3brVfPrpp2bOnDkmPDzcM5vtRz/6kVmyZImn/l//+lfToUMH8+yzz5rCwkLzxBNP1DtNPDw83Lzxxhvmo48+MuPGjQuoacRN6e/q1atNp06dzK5du7yuY+1yDhcuXDCPPPKIyc/PN8ePHzd//OMfzV133WVuu+02vwbzWk3t7/Lly80f/vAH89lnn5n9+/ebyZMnm5CQEHPo0CFPHStd31r33HOPmTRpUp3yQL++Fy5cMB9++KH58MMPjSTz3HPPmQ8//NB8/vnnxhhjlixZYn70ox956tdOE3/00UdNYWGh2bBhQ73TxBv6HfoLAcdPzp49a6ZMmWJuuukm43A4zMyZM73Wszl+/LiRZN59912v4zIzM018fLxxu911zvn73//eDB482Nx0002mS5cu5s477zQbN26st25ra2p/i4qKzD//8z+b7t27m+DgYNO3b1/z6KOPeq2DY4wxJ06cMGPGjDGdO3c2ERERZtGiRV7Tqv2lqf199913jaR6t+PHjxtjAuv6/upXvzK9evUynTp1MomJieb999/37BsxYoSZPn26V/1XX33V/NM//ZPp1KmT+da3vmX+53/+x2t/dXW1Wbp0qYmKijLBwcFm1KhR5siRI63RlUZpSn9vueWWeq/jE088YYwx5tKlS2b06NEmMjLSdOzY0dxyyy1m9uzZfv8wuFpT+rtgwQJP3aioKHPvvfeaAwcOeJ3PStfXGGMOHz5sJJk9e/bUOVegX99r/VtT28fp06ebESNG1Dlm8ODBplOnTqZPnz5ea/7Uauh36C82Y/w8hxgAAKCFsQ4OAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwHAIOAACwnP8P8t1tOqLMi58AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[-0.60494626,  0.39463717],\n",
    "       [-0.30378544,  0.35994518],\n",
    "       [-0.10199207,  0.8077354 ],\n",
    "       [-0.6891583 ,  0.56531763],\n",
    "       [-0.82298213,  0.12062865],\n",
    "       [-0.7219664 ,  0.36234102],\n",
    "       [-0.7760751 ,  0.44770154],\n",
    "       [-0.65878356,  0.02599421],\n",
    "       [-0.44636518,  0.05483197],\n",
    "       [-0.12357683,  0.67089474],\n",
    "       [ 0.59597784, -0.6578017 ],\n",
    "       [ 1.052702  ,  0.03756183],\n",
    "       [ 0.8736961 , -0.4984176 ],\n",
    "       [ 0.53515655, -0.71511005],\n",
    "       [ 0.78909314, -0.39306254],\n",
    "       [ 0.7054007 , -0.4269686 ],\n",
    "       [ 0.47644103, -0.65322984],\n",
    "       [ 0.24000564, -1.0623405 ],\n",
    "       [ 0.39430314, -0.70325773],\n",
    "       [ 0.21796265, -1.16941   ]], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "Y_ = [0 for i in range(10)] + [1 for i in range(10)]\n",
    "\n",
    "Y = np.array(Y_, dtype=np.float32)\n",
    "# show the plot\n",
    "for i in range(len(X)):\n",
    "    x,y = X[i]\n",
    "    label = Y[i]\n",
    "    if label > 0.5:\n",
    "        plt.plot(x, y, 'ro')\n",
    "    else:\n",
    "        plt.plot(x, y, 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uD9aHXN0FtGt"
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X)\n",
    "Y_train = torch.from_numpy(Y)\n",
    "target = Y_train[:, None]\n",
    "learning_rate = 0.3\n",
    "params = list(model.parameters())\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    model.zero_grad()\n",
    "\n",
    "    Y_tmp = model(X_train)\n",
    "    loss = criterion(Y_tmp, target)\n",
    "    print(f'i = {i}, loss = {loss}')\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update the gradient\n",
    "    for f in params:\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "activation = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach() > 0\n",
    "    return hook\n",
    "\n",
    "model.relu1.register_forward_hook(getActivation('relu1'))\n",
    "model.relu2.register_forward_hook(getActivation('relu2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "aJNqWEBIFzJ1",
    "outputId": "2006d335-ef03-4d0f-d352-0e21ea1f73e2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_tmp = model(X_train)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for i in range(len(X)):\n",
    "    x,y = X[i]\n",
    "    pred = Y_tmp[i][0]\n",
    "    if pred > 0.5:\n",
    "        plt.plot(x, y, 'ro')\n",
    "    else:\n",
    "        plt.plot(x, y, 'go')\n",
    "\n",
    "plt.plot(X[9][0], X[9][1], 'go', label=\"0\")\n",
    "plt.plot(X[-1][0], X[-1][1], 'ro', label=\"1\")\n",
    "\n",
    "fig.suptitle('Simple data distribution')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCjVMtjXHXY6",
    "outputId": "de2a236f-c690-455b-89ed-35e1a882c37f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(torch.from_numpy(X[0]))\n",
    "activation['relu1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ciHIldrGvMb"
   },
   "outputs": [],
   "source": [
    "Refinement:\n",
    " For given area, find adversarial example. Refine that region."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
